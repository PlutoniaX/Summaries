

The text is the introduction to the book "Atlas of AI" by Kate Crawford. 

The book explores the power, politics, and planetary costs of artificial intelligence. 

The text begins with a story about a horse named Hans who was known for his ability to solve math problems, tell time, and spell words. 

Copyright information, publishing details, and a table of contents are included in the text.

Clever Hans was a horse trained by a retired math teacher named Wilhelm von Osten, who taught him to count and tap out simple sums accurately. Hans became a viral sensation, but many people were skeptical about his abilities, so the German board of education launched an investigative commission. After extensive questioning, the commission found no evidence of deception, but they discovered that Hans was responding to unintentional cues from his trainer, such as body language and subtle facial expressions. This phenomenon came to be known as the Clever Hans effect and is now recognized as an example of how humans can unwittingly influence animal behavior.

The story of Clever Hans, a horse trained to perform arithmetic, reveals the influence of unintentional cues on animal behavior. An investigative commission found that Hans was responding to subtle cues from his trainer, such as body language and facial expressions, and not exhibiting extraordinary intelligence. This phenomenon became known as the Clever Hans effect and is now recognized as a cautionary reminder in fields such as psychology and machine learning. The story also highlights the complex mechanisms by which biases find their way into systems and how people become entangled with the phenomena they study.

The book explores how intelligence is made and the traps it can create. The story of Clever Hans, a horse trained to follow cues, demonstrates the validation required from multiple institutions and interests to construct intelligence. Two myths about intelligence are debunked, namely that non-human systems are analogues for human minds and that intelligence exists independently of social, cultural, historical, and political forces. The concept of intelligence has been used to justify relations of domination, and these myths are particularly strong in the field of artificial intelligence.

The idea of AI systems being like human intelligence has been present since the early days of computing, with some arguing that machines can think like humans. This idea has been challenged, with Joseph Weizenbaum arguing that it is too simplistic a notion of intelligence. The differences between human and machine tasks were debated in a landmark lecture series in 1961, with John McCarthy arguing that the differences were illusory and Hubert Dreyfus arguing that the brain might process information differently than a computer. Dreyfus later pointed out that human intelligence and AI are fundamentally different.

The article discusses the limitations of artificial intelligence (AI) and its narrow perspective on intelligence, which is rooted in the belief that the mind is like a computer. AI relies on explicit and formalized data, while human intelligence relies heavily on unconscious and subconscious processes. Despite the hype around AI, the author argues that it still has a limited understanding of intelligence and is disconnected from the material world. The story of Clever Hans, a horse taught to mimic tasks within a very constrained range, reflects this limited perspective. The article calls for a broader understanding of intelligence beyond AI's narrow view.

The article explores various definitions of artificial intelligence (AI), including technical ones that focus on neural networks and more general ones that emphasize the attempt to build intelligent entities capable of rational action. The author argues that how AI is defined affects how it is understood, measured, valued, and governed. They propose a broader understanding of AI that recognizes its embodied and material nature, made up of natural resources, fuel, human labor, infrastructures, histories, and classifications. AI systems are not autonomous or rational and require extensive training with large datasets or predefined rules.

Artificial intelligence is not solely a technical domain but is a registry of power shaped by economic, political, cultural, and historical forces. It reflects and produces social relations and understandings of the world. The term AI is often used and rejected in ways that keep its meaning in flux. The field has been focused on technical aspects, but it includes politics, labor, culture, and capital. Machine learning is a range of technical approaches that are also social and infrastructural.

To understand how AI is political, we need to go beyond technical aspects and consider what is being optimized and for whom. An atlas can help us understand how AI is made by presenting a particular viewpoint of the world that combines aesthetics and knowledge. It offers the possibility of linking disparate pieces differently and reediting them to reread the world. A cartographic approach can be helpful in understanding the implications of AI choices.

Maps can be purposeful and useful, offering open pathways for new connections, but they can also be used for domination and power. To understand the empires of artificial intelligence, we need a topographical approach that accounts for the shifting tectonics of power, including the states and corporations that drive and dominate it, extractive mining, data capture, and exploitative labor practices. The AI industry aims to capture the planet in a computationally legible form, creating centralized maps of human movement, communication, and labor. AI needs to be understood in a wider context by walking through different landscapes of computation and seeing how they connect.

AI aims to be the dominant way of seeing, centralizing power in the field and determining how the world is measured and defined. This book offers a partial account of AI and its ideologies, exploring lesser-known landscapes of computation and illuminating the subjective and political aspects of mapping. Transparency is an impossible goal in the interconnected systems of AI, and understanding its role requires engaging with its material architectures, contextual environments, and prevailing politics. The author's thinking has been informed by science, media studies, geography, and history.

The author of the text has experience in technology studies, law, and political philosophy, and has worked in both academia and an industrial AI research lab. They acknowledge the contributions of various scholars and authors who have influenced their perspective on sociotechnical systems and the politics of technology. The author notes that their focus is on the AI industry in Western centers of power, but their aim is not to create a complete global atlas. They emphasize that the expanding reach of AI systems is not inevitable and that the underlying visions of the AI field are constructed from a particular set of beliefs and perspectives.

The contemporary atlas of AI is created by a small and homogenous group of people in a wealthy industry, and is a political intervention rather than a neutral reflection of the world. The book explores how AI is conceptualized and constructed, the politics contained in the way these systems map and interpret the world, and the social and material consequences of including AI in various decision-making systems. It takes an expanded view of AI as an extractive industry that relies on exploiting energy and mineral resources, cheap labor, and data at scale. The book goes on a series of journeys to observe this in action.

The book explores the environmental and human impact of AI development, starting with the extractive politics of mining rare earth minerals needed for computation. It also highlights the human labor involved in building and training AI systems, including digital pieceworkers, warehouse employees, and meat laborers. The coordination of human and robotic labor has always involved controlling bodies in space and time, and AI technologies both require and create these conditions.

The book discusses the increasing use of granular and precise mechanisms of temporal management in AI, which requires detailed information about people's activities. It also highlights the role of data in AI development, including the harvesting of personal data for training datasets, which raises ethical and methodological concerns. The book examines the practices of classification in AI systems, including problematic assessments of human identity, which can enforce hierarchies and magnify inequity. The regime of normative reasoning in machine learning can take shape as a powerful governing rationality, and the book explores affect recognition in Papua New Guinea.



 Chapter 5 explores the idea that facial expressions reveal a person's inner emotional state, but there is controversy around the use of AI in affect recognition systems for hiring, education, and policing. 

 Chapter 6 examines how AI is used as a tool of state power and how military logics have shaped AI systems, now present in municipal government, further skewing the relationship between states and subjects. 

 The concluding chapter assesses how AI combines infrastructure, capital, and labor, built with the logics of capital, policing, and militarization, widening existing asymmetries of power through abstraction and extraction.



 Artificial intelligence is an idea, infrastructure, industry, exercise of power, and way of seeing, shaped by highly organized capital, extraction, and logistics, and with a global reach. 

 AI is malleable, messy, and promiscuous, and encompasses a wide range of devices, systems, and companies, making it useful to consider how its elements are deeply interconnected. 

 Contending with AI's many aspects is important, as it is reshaping the Earth and shifting how the world is seen and understood, but calls for data protection, labor rights, climate justice, and racial equity can challenge its logics and inform new conceptions of planetary politics.

Artificial intelligence is now a powerful player in shaping knowledge, communication, and power, impacting epistemology, justice, social organization, politics, culture, and identity. The concentration of industrial capital and defunding of social welfare systems has accelerated this process, and AI is a political, economic, cultural, and scientific force. It is essential to ask hard questions about the production and adoption of AI and its politics, and to consider whose interests it serves.

The use of AI raises questions about whose interests it serves and who bears the greatest risk of harm. Addressing foundational problems of AI requires connecting issues of power and justice, from epistemology to climate change. Silicon Valley is a key site for the empires of AI, including Google, Apple, Lockheed Martin, and Facebook.

The author travels from Silicon Valley to Silver Peak, Nevada to learn about artificial intelligence. Along the way, they pass by various locations of historical and industrial significance, including the Lawrence Livermore National Laboratory and the world's biggest ammunition depot. Silver Peak is a small mining town that was almost abandoned in 1917 but now has something of great significance related to artificial intelligence.

Silver Peak, Nevada is home to a lithium mine where the valuable mineral is extracted and processed to make AI technology. The history of mining in the area, and its destructive impact on the environment and local communities, is often overlooked in discussions of technological progress. Mining for AI is part of a long tradition of exploiting natural resources at a great cost to people and the planet.

Georgius Agricola observed in 1555 that mining causes greater detriment than the value of the metals produced. However, the true value of a wilderness, clean water, air and health was never estimated, leading to the rapid extraction of everything. As a result, the Central Valley was decimated and the mines had no respect for any rights in California. San Francisco drew enormous wealth from the mines, but the city's populace remained ignorant of the destruction caused. The city's new buildings used the same technology as the mines, and skyscrapers are now considered inverted minescapes.

The extraction of substances like white lithium crystal, known as "gray gold", has replaced ore mining as the new supreme interest in San Francisco, with tech giants such as Apple, Microsoft, Amazon, Facebook, and Google having offices there. However, the tech boom has also led to a high rate of street homelessness, which the UN has called an unacceptable human rights violation. The benefits of extraction have been captured by the few, with violence, pollution, extinction, and depletion undergirding the constant drawdown of minerals, water, and fossil fuels. The effects of large-scale computation are also explored.

The text discusses the planetary scale of computational extraction and the mining boom for lithium, a crucial element for rechargeable batteries used in technology such as smartphones and electric cars. The only operating lithium mine in the United States is located in Silver Peak, which is of intense interest to tech tycoons like Elon Musk. Lithium batteries have a limited lifespan and are discarded as waste. The Tesla Gigafactory is located about 200 miles north of Silver Peak.

Tesla is the largest consumer of lithium-ion batteries in the world, using over half of the planet's total consumption. They are more of a battery business than a car company. The imminent shortage of critical minerals like nickel, copper, and lithium poses a risk for the company, making the lithium lake at Silver Peak desirable. The negative impact of the battery supply chain on the environment and affected communities is significant. Global computation and commerce rely heavily on rechargeable lithium-ion batteries, which undergird the internet and every commerce platform that runs on it.

The use of cloud computing for various aspects of modern life has resulted in a significant demand for Earth's resources, including lithium and crude oil. The supply chain required to power AI systems goes beyond technical aspects, reaching into capital and labor. Media and technology are considered geological processes, reflecting the depletion of nonrenewable resources to drive current technologies. AI systems extract Earth's geological history to serve contemporary technological time, building devices that are often not designed to last.

The short lifespan of devices fuels obsolescence cycles, leading to unsustainable extraction practices and e-waste dumping. AI systems rely on many forms of exploitation and consumption of energy, which extract far more from humans and the planet than is commonly known. The tech industry's practices echo the extractivism on which San Francisco was built. The mineralogical layer of AI involves extracting materials such as lithium from sites like the Salar in Bolivia. These legacies of human and environmental damage powering the tech industry extend beyond the United States.

AI is dependent on minerals from various locations, including Congo, Mongolia, Indonesia, and Western Australia. The shortage of these materials, including rare earth elements such as dysprosium and neodymium, poses a high supply risk to manufacturers, including the tech sector. Extracting these minerals often leads to local and geopolitical violence.

The extraction of rare earth minerals for AI technology often involves conflict, exploitation, and modern slavery. Legislation such as the Dodd-Frank Act regulates the use of conflict minerals, but the term masks the suffering and death associated with their extraction. Companies like Intel and Apple have faced criticism for only auditing smelters and not the actual mines, and even their conflict-free certifications are under question. Supply chains for AI technology are complex and involve tens of thousands of suppliers in multiple countries.

The complex supply chains for minerals used in electronics manufacturing make it difficult to trace their provenance and ensure conflict-free sourcing. This allows companies plausible deniability for any exploitative practices that drive their profits. Ignorance of the supply chain is baked into capitalism and serves as a well-practiced form of bad faith. While conflict minerals receive attention, the harms of mining writ large, including environmental destruction and human suffering, are often overlooked.

The extraction of rare earth minerals in China has led to the creation of toxic waste lakes and environmental damage due to the hazardous process of dissolving them in large volumes of acid. Although rare earth minerals are relatively common, the usable minerals to waste toxins ratio is extreme, with only a small percentage of mined material containing valuable elements. China's market domination in rare earth minerals is due to its willingness to take on environmental damage.

The production of rare earth elements in China produces acidic water and radioactive residue, while tin mining in Indonesia, the world's second-largest producer, is carried out by unregulated gray-market miners who damage the landscape, forests, and coral reefs, and have caused fatalities. These industries supply major technology companies like Apple, Tesla, and Amazon.

To understand the full supply chains of AI, we must look at global patterns and recognize the interconnectedness of different regions and histories. Transatlantic telegraph cables, which are essential for data transfer, are also a product of colonialism and the exploitation of Southeast Asian trees for their gutta-percha sap. Workers in Malaysia were paid little for the dangerous work of felling trees and collecting latex, which was processed and sold through Singapore's trade markets to the British market for use in submarine cable sheaths.

Submarine cables were seen as an efficient mode of communication and control over colonies. The demand for gutta-percha, a material used in cables, led to ecological disaster and extinction of the Palaquium gutta tree. The relations between technology and its materials, environments, and labor practices are interwoven. Algorithmic computation emerged out of desires to manage and control war, population, and climate change and was imbricated with race science. Currently, large-scale AI systems are driving forms of environmental, data, and human extraction.

Statistics were seen as a way to examine the effects of selective processes on race. Regression techniques, standard deviation, and correlations became dominant tools for interpreting social and state information. Algorithmic computing, computational statistics, and AI were developed to address social and environmental challenges but are now used to intensify industrial extraction and exploitation. The environmental impact of AI is often overlooked, and the tech industry's sustainability initiatives may not reflect their actual carbon footprint. It takes a significant amount of energy to run the computational infrastructures of major companies, and the carbon footprint of their AI systems is considerable.

The rapid growth of cloud computing platforms is leading to environmental damage as they convert water and electricity into computational power. Despite efforts to increase energy efficiency and use renewable energy, the carbon footprint of computational infrastructure has already matched that of the aviation industry and is increasing at a faster rate. The energy consumption of training AI models is still an emerging area of investigation, but initial findings suggest that it comes at a high cost to the planet, with even a single NLP model producing more than 660,000 pounds of carbon dioxide emissions. Addressing this energy-intensive infrastructure has become a major concern for the industry.

The energy consumption of AI models used by tech companies like Apple and Amazon is unknown, but likely much higher than estimated, as they operate at a commercial scale using internet-wide datasets. The belief that bigger is better in AI training runs has driven a steep increase in energy consumption, as developers repeatedly find ways to use more chips in parallel. Data centers are among the world's largest consumers of electricity, and the tendency toward compute maximalism has profound ecological impacts. Some corporations are responding to growing alarm about energy consumption, but the wider local and environmental price of burning computation cycles to create incremental efficiencies is not being fully addressed.

Microsoft, Google, and Amazon aim to become carbon negative by 2030, but workers urge for reduction in emissions. These companies license their AI platforms, engineering workforces, and infrastructures to fossil fuel companies. China's data center industry heavily relies on coal, emitting large amounts of CO2, while the country's biggest tech companies need to disclose energy use data and scale up clean energy procurement. The impact of coal-fired power exceeds national boundaries. The National Security Agency's data center in Utah is a symbolic power of the next era of government, and the history of water use in the US is full of battles and secret deals.

The National Security Agency's data center in Utah looks nondescript but its water usage is contested due to its estimated consumption of 1.7 million gallons of water per day. Activists created handbooks to encourage ending support of water and energy to surveillance, but the city of Bluffdale had already made a deal with the NSA to sell water at rates below average. The geopolitics of water are now intertwined with data centers, computation, and power. Data centers are far from major population hubs and affect the environment and climate in unrecognized ways.

The growth of AI requires expanding resources and logistics, including the global logistical machines that move materials and devices around the planet. Standardized cargo containers have enabled the modern shipping industry, which allows for the envisioning and modeling of the planet as a single massive factory. However, cargo shipping disguises far larger external costs and neglects physical realities and ethical considerations.

The shipping industry, which is crucial for the logistics of AI infrastructure, has significant environmental and ethical costs that are often overlooked, leading to "sea blindness." Container ships emit high levels of pollutants, lose containers at sea, and often subject workers to poor working conditions. The growth of cloud-based computation has paradoxically driven an expansion of resource extraction. Understanding the true costs of increasing automation requires factoring in these hidden costs and working against the usual technological imaginary that is untethered from earthly matters.

Artificial intelligence (AI) is a megamachine that relies on industrial infrastructures, supply chains, and human labor, stretching around the globe. It involves manufacturing, transportation, physical work, data centers, transmission signals, and computational cycles. The production, manufacturing, and logistics of AI come at a cost, and there are connections between cities, mines, companies, and supply chains. The concept of the megamachine was developed in the late 1960s to illustrate how all systems, no matter how immense, consist of the work of many individual human actors.

The mines that power AI are not just in discrete locations but scattered across the globe, forming a planetary mine. Industries in the AI system chain conceal the ongoing costs of their work, and the complexity of building AI systems is obscured by intellectual property law and logistical and technical complexity. The aim is not to make these complex assemblages transparent but to connect across multiple systems and understand how they relate to each other. The environmental and labor costs of AI must be contextualized within the practices of extraction and classification woven throughout everyday life to work toward greater justice.

Blair was a town built by a mining company near Silver Peak in Nevada, with a hundred-stamp cyanide mill and a railroad. The town thrived for a while, but with excessive mining, the cyanide poisoned the ground and the gold and silver seams faltered. By 1918, Blair was deserted. Today, it is a ghost town, marked by ruins. Silver Peak, which is currently a lithium mine, will also become a ghost town soon once the lithium pools are depleted, and the batteries made from it end up in landfills.

The Amazon fulfillment center in Robbinsville, New Jersey, is a vast warehouse with dozens of time-clock signs that monitor and tally every second of work by associates. Workers are only allowed to be off-task for 15 minutes per shift, with a 30-minute unpaid meal break. The warehouse features robots that move heavy shelving units laden with products, presenting an effortless efficiency that carries, rotates, advances, and repeats. The warehouse has 14 miles of conveyor belts moving packages, making it a major distribution center for smaller objects.

The article describes the use of robotics in Amazon's logistics operations and the toll it takes on human workers who perform the fiddly tasks that robots cannot. Workers are under pressure to meet picking rates, leading to injuries and reliance on painkillers from vending machines. While humans are necessary for getting items delivered to consumers, they are not the most valuable component of Amazon's operations. The basic unit of measurement at Amazon is the brown cardboard box. Workers are required to exit through metal detectors at the end of the day as an anti-theft measure.

The article discusses Amazon's focus on efficient delivery and how it is achieved through machine learning algorithms that determine the best box size and type to prevent breakages. Workers are constantly forced to adapt to new processes, and their bodies are run according to computational logics. Many companies are investing in automation to extract more labor from fewer workers. The article explores the trade-offs made in the pursuit of automated efficiency and how AI systems are changing the experience of work through increased surveillance, algorithmic assessment, and time modulation.

The article discusses how humans are increasingly treated like robots in the workplace and how artificial intelligence is rooted in the exploitation of human labor. It highlights the need to understand the past and present experience of workers to understand the future of work in the context of AI. The article explores the prehistories of workplace AI, including the manufacturing assembly line, service industries, and secretarial labor, which have been automated since the 1980s. The article suggests that the model of increased conformity, standardization, and interoperability for products, processes, and humans alike has emerged from the lineage of the mechanized factory.

The article discusses how white-collar employees are increasingly subjected to workplace surveillance, process automation, and blurred boundaries between work and leisure time. The expansion of AI systems and process automation is seen as a return to older practices of industrial labor exploitation, where work tasks were subdivided into smaller actions requiring minimal skill but maximum exertion. The article highlights the power asymmetry between workers and employers in this forced engagement, where workers are expected to re-skill and keep up with new technical developments. The article suggests that the current expansion of labor automation continues the historical dynamics inherent in industrial capitalism, where more value is transferred to employers.

The history of workplace AI can be traced back to the Industrial Revolution, where Adam Smith observed that dividing manufacturing tasks into smaller steps led to improved productivity and mechanization. Mechanization combined with an abundance of fossil fuels drove a massive increase in production, transforming the role of labor in the workplace. Machines became the center of productive activity, shaping the speed and character of work, and turning workers into adaptors of machine needs and rhythms. Karl Marx noted that automation abstracts labor from the production of finished objects and turns a worker into a part of the machine.

During the Industrial Revolution, workers became integrated with machines, turning into an appendage of the machine, and viewed as a raw material to be managed and controlled. This led to strict time disciplines and greater synchronization of work, with time being seen in moral and economic terms. Labor movements began advocating for reducing the working day, leading to a struggle over time. To maintain an efficient and disciplined workforce, new systems of surveillance and control were invented, such as the inspection house.

The inspection house, developed by Samuel Bentham in 1780s Russia, allowed supervisors to monitor untrained subordinates and for Bentham to keep an eye on the supervisors. It was designed to address issues of poor working habits and ill-discipline. The redesign of the inspection house became the inspiration for the panopticon, a design for a model prison with a central watchtower. The panopticon was originally designed as a workplace mechanism before being conceptualized for prisons. The inspection house was part of a strategy to modernize rural Russia and transform the peasantry into a modern manufacturing workforce.

The practices of observation and control in the workplace have a long history, including Potemkin villages in Russia and the overseer system on colonial slave plantations in the Americas. Modern workplaces now use surveillance technologies to monitor and analyze employee data to make predictions about success, divergent behavior, and organizing efforts. This includes tracking movements with apps, analyzing social media, and using machine learning or algorithmic systems.

Many AI systems rely on underpaid workers to build, maintain, and test them, and this hidden labor is often exploitative and poorly compensated. This labor takes many forms, from supply-chain work to on-demand crowdwork. Workers perform repetitive digital tasks such as labeling training data and reviewing harmful content, but rarely receive credit for their contribution. Despite often having specialized education in science and technology, many of these workers earn below their local minimum wage.

The development of AI systems relies heavily on underpaid and exploitative labor, such as crowdworkers and content moderators, which is often ignored or disregarded by clients and employers. This labor is essential for refining AI systems quickly and cheaply, but its exploitation raises ethical questions about fair compensation and treatment. Despite concerns about psychological trauma and exploitation, there has been little debate on the issue, as recognizing and compensating human workers fairly would make AI more expensive and less efficient. Some companies even ask workers to pretend to be AI systems.

Many AI systems, such as x.ai and Facebook's personal assistant M, rely on human labor behind the scenes to sustain the illusion of automation. This practice, known as Potemkin AI or fauxtomation, creates facades of automated systems while coordinating human work in the background. This is a core logic of how AI works until there is another way to create large-scale AI without extensive human work. Fauxtomation can be seen in self-service kiosks and self-checkout systems, where the data-entry labor is simply relocated from a paid employee to the customer.

Many valuable automated systems rely on underpaid human labor, creating an illusion of AI while dispersing and relocating labor in space and time. This approach scales cost reductions and profit increases while obscuring the true labor costs and alienating workers from the results of their work. Fauxtomation perpetuates a tradition of exploitation and deskilling, with workers competing for subsistence wages and facing the possibility of being replaced by other crowdworkers or more automated systems.

The Mechanical Turk was a mechanical man built in the 18th century that could play chess, but it was actually operated by a hidden human. Amazon named its crowdsourcing platform after the Mechanical Turk, even though the illusion of autonomy and intelligence perpetuates the association with trickery and racism. Amazon's motivation for building the platform was to enlist humans to fill gaps in its artificial intelligence systems, and now it connects businesses with anonymous workers who improve on AI systems by checking and correcting algorithmic processes.

Many examples of Potemkin AI exist, which hide the combination of human and machine labor in interactions with AI systems. Some are visible, like human operators in self-driving cars, while others are less so, like web-based chat interfaces that do not disclose whether a response is from a machine or a human operator. The myth of AI as affordable and efficient depends on exploitation, including mass unpaid labor to fine-tune AI systems for the richest companies on earth. Contemporary AI is not truly artificial or intelligent, and it relies on the exploitation of human labor in various forms, including physical, factory, cybernetic, crowdsourced, and unpaid immaterial work.

Charles Babbage developed the idea of the Difference Engine and Analytical Engine, mechanical computers that used punch cards for instructions. He also had an interest in liberal social theory and saw the industrial corporation as an analogue to a computational system. He argued that through computation, surveillance, and labor discipline, it was possible to enforce higher degrees of efficiency and quality control. Babbage's economic thought diverged from Adam Smith's in that he believed value in a factory was derived from investment in design rather than the cost of labor. Babbage's vision of computation and worker automation has only recently become possible with the adoption of artificial intelligence in the workplace.

Babbage viewed labor as a problem that needed to be contained by automation, with little consideration of the human costs of this automation. His idealized machinery aimed primarily to maximize financial returns to plant owners and investors. The factory was seen as a rational calculating machine with an untrustworthy human labor force. Today's proponents of workplace AI prioritize efficiency, cost-cutting, and higher profits over improving the working lives of employees. The standard business model of for-profit companies prioritizes shareholder value over diversity, complexity, and interdependence.

Automation and alternative work jobs are on the rise in the US, leading to longer working hours, insecure positions, and lower pay. The meat-packing industry in Chicago was one of the first to adopt automation and division of labor, resulting in low-paid and easily replaceable workers. Upton Sinclair's novel, The Jungle, brought attention to food safety rather than workers' conditions, leading to the passing of the Meat Inspection Act in 1906.

Powerful institutions respond to critique by accepting regulation at the margins but leaving untouched the underlying logics of production, whether in the meat-packing industry or in the case of facial recognition technology. Frederick Winslow Taylor's scientific management approach focused on quantifying workers' movements with the aim of maximum output at minimal cost, exemplifying the domination of clock time. Foxconn, which produces Apple products, is an example of how workers are reduced to mere cogs in the production process.

The text discusses how time is used as a means of controlling human bodies in the workplace, particularly in service and fast-food industries. Workers are subjected to strict time management protocols that leave no room for error and are tracked by algorithmic scheduling systems. The use of time in this way has been criticized as leading to significant wage theft and dehumanizing language from management.

The text highlights how the use of algorithms to determine work schedules in service and fast-food industries can result in significant wage theft, as workers may be sent home or not given enough hours to earn a decent wage. This approach to time management traces its roots back to the Fordist factory system, which focused on minimizing cycle times and reducing individual craft in favor of standardized processes. Today, employers can passively surveil their workforce through electronic time clocks and timing devices.

Many companies are using surveillance technology to monitor their employees, including tracking body temperature, physical distance, and web browsing habits. WeWork and Dominos Pizza are among those who have implemented such systems. These surveillance tools are often used to create algorithmic scheduling systems, analyze employee performance, and generate data for data brokers. The demographic makeup of Silicon Valley, which is young, male-dominated, and focused on productivity, contributes to the development of such tools. This reinforces a vision of a standard worker that is masculinized, narrow, and reliant on the unpaid or underpaid care work of others.

Technological forms of workplace management have led to the granular tracking of time, with protocols like NTP and PTP establishing clock hierarchies across networks. The master-slave terminology used in these protocols, which originated in astronomy and computing, has problematic implications and echoes of pre-Civil War discourse on runaway slaves. This terminology has been removed from some programming languages.

Google's Spanner is a globally distributed database infrastructure that synchronizes time across millions of servers in hundreds of data centers using a proprietary form of time protocol called TrueTime. TrueTime establishes trust relationships between local clocks of data centers to synchronize events in a determinate sequence across a wide area network, even in the presence of clock drift on individual servers. If uncertainty is large, Spanner slows down to wait out the uncertainty. This embodies the fantasy of slowing down time and bringing the planet under a single proprietary time code.

Google's TrueTime creates a shifting timescale controlled by a centralized master clock, representing a proprietary form of universal time. Proprietary forms of time have existed in history, such as railroad magnates in the 19th century adopting true time to avoid inconvenience, and the telegraph enabling a new form of monopoly capitalism and erasing local forms of timekeeping. The telegraph was dominated by Western Union, one of the first industrial monopolies.

The development of the telegraph and transatlantic cable allowed imperial powers to maintain more centralized control over their colonies and made time a central focus for commerce. The ordering of time through infrastructure creates new logics of information at a planetary level, and such power is necessarily centralizing. Resistance to centralized time has historically taken the form of defiance and sabotage in various industries. Although there will always be ways to resist imposed temporality, the privatized time zones of data centers are the latest example of infrastructural ordering of time.

Defining and controlling time has always been a strategy for centralizing power, from clocks for churches to data centers. The tech sector creates a smooth global terrain of time to strengthen and speed its business objectives, while workers have found ways to resist even as technology forces greater surveillance and company control. Amazon carefully controls what the public sees in its fulfillment centers, but signs of unhappiness and dysfunction are still present, such as complaints about working conditions written on whiteboards. Artificial intelligence systems have allowed for greater exploitation of distributed labor, and defining time becomes more difficult with algorithmic and video monitoring.

The article highlights the discrepancies between Amazon's public statements and the working conditions reported by its employees. Although Amazon claims that unions are unnecessary because employees can easily communicate with their managers, a live feed of worker messages on a screen in one facility showed complaints about arbitrary scheduling changes, missed family occasions, and a lack of human treatment. Workers have reportedly suffered from stress, injuries, and illness due to the high productivity metrics at Amazon fulfillment centers, despite the company's efforts to recruit them with incentives such as free busing. Advocates like Abdi Muse are pushing for better working conditions for Amazon warehouse workers.

The article discusses the working conditions of Amazon warehouse workers, including high productivity metrics and fears of being fired for underperformance. Despite Amazon's discouragement of unions, informal worker groups have been springing up across the US, staging protests and walkouts to demand better working conditions. Workers in Eagan, Minnesota, walked off the job demanding increased wages and weight restrictions on boxes, while workers in Sacramento protested the firing of an employee who had gone over bereavement leave. Despite negotiations, Amazon representatives refused to budge on the core issue of productivity metrics, which were set by executives and tech workers in Seattle.

Workers at Amazon are organizing across different factories and sectors to address the issues of power and centralization represented by the relentless rhythm of the rate itself. The fight for time sovereignty is not new, and workers are fighting against the logics of production and the order of time they must work within. Technologically driven forms of worker exploitation are widespread, and cross-sector solidarity in labor organizing is necessary to win victories for worker rights and protections.

Labor organizers are fighting AI-driven systems of extraction and surveillance as a unified front, with solidarity built around tech work. However, there are risks in centering tech workers and technology in more generalized labor struggles, as all workers are subject to extractive technical infrastructures. The broader goal should be producing more just conditions for every worker, and everyone has a stake in the future of work. A collection of mug shots of a woman across multiple arrests over many years of her life is used to illustrate the impact of surveillance and the carceral state on individuals.

The National Institute of Standards and Technology (NIST) maintains several datasets, including a biometric database used for testing facial recognition software. NIST was established in 1901 to create standards for measurement and infrastructure, and now includes developing standards for artificial intelligence. NIST collaborated with the FBI on automated fingerprint recognition and biometric standards after the September 11 attacks. The mug shot images in the database are extensive and were originally intended for law enforcement but now also used for border control.

NIST's Special Dataset 32 contains thousands of mug shots of deceased people who had multiple arrests, and are presented as data points without context or names. Mug shots were originally used to identify individuals in law enforcement, but now serve as a technical baseline for testing AI facial recognition systems. The history of police photography includes two approaches, one for identification and the other for detecting a biologically determined criminal type. Mug shots used as training data fine-tune an automated form of vision to detect the basic mathematical components of faces.

The NIST Multiple Encounter Dataset provides standardized facial images for algorithmic accuracy competitions. People depicted in the photographs have no control over their use and are rarely considered. The mug shots are used purely to refine tools for face recognition and are seen as a shared technical resource. The dataset contains images of people who show signs of enduring violence, but these signs are difficult to interpret. The use of these images raises concerns about privacy invasion and the lack of consent.

The tech sector's unswerving belief that everything is data and can be used has normalized the use of images, such as mugshots, for AI training. The personal, social, and political meanings of these images are assumed to be neutralized, and they are treated as data to improve technical performance. However, these images represent personal histories, structural inequities, and injustices in the policing and prison systems. The presumption that these images can serve as apolitical, inert material influences how machine learning tools see and is a core premise in the ideology of data extraction.

Computer vision systems focus on aggregate data and fail to consider the social and historical context surrounding individual images such as faces. This approach dehumanizes individuals and ignores the institutional, personal, and political contexts of their images. Mug shot collections are used as a resource to train facial recognition systems, which are then used to monitor and detain more people. The capture of digital material for AI production has become fundamental to the field, but there is a need to examine how data is acquired, understood, and used in machine learning and the ways in which training data limits AI interpretation of the world.

The AI industry's standard approach of mass data harvesting has resulted in a metamorphosis where all forms of image, text, sound, and video are considered just raw data for profitable computational intelligence. This transformation is defined by the logic of extraction, similar to the relationship to the earth and human labor. Machine learning systems demand massive amounts of data, particularly for the complex and relational endeavor of interpreting images in computer vision. It is common practice to scrape thousands or millions of images from the internet and use them as a foundation for training computer vision systems.

AI developers use training datasets, which are collections of labeled data examples, to train supervised machine learning algorithms such as learners and classifiers. The accuracy of the algorithm's predictions depends on the quantity and quality of labeled data. There are different types of machine learning models such as neural networks, logistic regression, and decision trees that are chosen based on the purpose of the system being built. The development of machine learning systems involves collecting, labeling, and training a neural network on thousands of labeled images. However, if the training data is biased or incomplete, it can affect the accuracy of the algorithm's predictions.

Training datasets are at the core of most machine learning systems, serving as the primary source material for their predictions and defining their features. Machine learning algorithms are constantly tested against each other in competitions using benchmark datasets. Training sets inherit learned logic from earlier examples and give rise to subsequent ones, forming an expanding encyclopedia of knowledge. The epistemic boundaries governing how AI operates are shaped by training datasets.

The limits of how AI can see the world are created by training data which is a brittle form of ground truth. In 1945, Vannevar Bush predicted the need for enormous amounts of data to fuel advanced arithmetical machines. Key punch operators, often dismissed as input devices, were important to crafting data and making systems work. The relationship between data and processing machinery was already being imagined as one of endless consumption.

In the 1970s, expert systems were used for AI but proved impractical in real-world settings, leading to the use of probabilistic or brute force approaches in the mid-1980s. The speech recognition group at IBM Research used statistical methods focusing on word frequency rather than linguistic features, requiring an enormous amount of training data. This shift reduced speech to data, which could be modeled and interpreted without linguistic understanding, leading to a radical change in how speech was viewed.

The IBM Continuous Speech Recognition group relied on statistical models and large datasets to improve their technology. The scarcity of computer-readable text made finding training data challenging, and they searched for it in various sources. The group's approach mirrored a science fiction story in which a man needs to recreate the entire universe to program an autonomous poetry machine. Eventually, the group found their dataset from an unlikely source, a federal antitrust lawsuit against IBM.

The government's digitization of deposition transcripts in an antitrust lawsuit against IBM inadvertently created a corpus of a hundred million words by the mid-1980s. Other groups, such as the Penn Treebank Project and the Enron fraud investigations, also collected large datasets for natural language processing purposes. The emerging text collections built on earlier collections and represented the biases of the individuals involved. Despite their popularity, these datasets are rarely examined closely. The reliance on training data anticipated a new way of constructing and understanding language.

Language models and natural language processing were transformed by the use of large text corpuses, but the assumption that all training data is interchangeable is flawed. Language is not neutral and the biases and cultural context of the text collected can influence the models that rely on it. There are no standardized practices for noting where data comes from or its biases, and languages with less available data are often left behind. The origins of underlying data in a system are significant and there is a need to understand and communicate these biases.

The Face Recognition Technology (FERET) program was funded by the Department of Defense to develop automatic facial recognition for intelligence and law enforcement. FERET created a training set of portraits of over a thousand people, becoming a standard benchmark to compare approaches for detecting faces. FERET was used for automated searching of mug shots, monitoring airports and border crossings, and searching databases for fraud detection. The primary testing scenarios involved locating the closest matches from a large gallery of known individuals or identifying known individuals from a large population of unknown people. The machine-readable photographs are high-resolution and captured in the style of formal portraiture.

The FERET program collected photographs of people's faces in the 1990s with their consent for the purpose of testing facial recognition algorithms. It was useful for this purpose, but lacked diversity in its collection. After 9/11, interest in facial recognition increased and automated vision systems rapidly expanded. The internet provided a wealth of images for AI research, leading to the development of ImageNet.

Social media platforms provide an abundance of images and text for AI training sets. Trillions of lines of text and millions of photos are uploaded daily, making it a natural resource for machine learning. The tech industry gains power from the vast amounts of data, while computer science labs seek ways to obtain similar advantages. Proprietary troves of data are highly valuable and rarely shared. New ideas are emerging, such as combining internet-extracted images and text with the labor of low-paid crowdworkers to create training sets.

In 2006, Professor Fei-Fei Li conceptualized the creation of ImageNet, a large dataset for object recognition. The team harvested over 14 million images from the internet and organized them into more than 20,000 categories for use in training and benchmarking object and image recognition algorithms. Ethical concerns about using personal images were not mentioned in research papers, and labeling the images into categories was a major concern.

To add images to the ImageNet dataset, undergraduate students were initially hired but it would have taken over 90 years to complete the project. A student introduced Professor Fei-Fei Li to Amazon Mechanical Turk, which provided access to a distributed labor force to sort and label images at low cost. Offensive and racist labels were given to some images, and the dataset grew into a benchmark for machine learning. The approach of mass data extraction without consent and labeling by underpaid crowdworkers became standard practice, marking a shift away from consent-driven data collection.



Staged photo shoots for datasets led to the presumption that internet content was free to take without agreements, releases, or ethics reviews. 

Unethical practices of data extraction emerged, such as secretly capturing photos of students and faculty to train facial recognition systems. 

The DukeMTMC project, funded by the US Army and National Science Foundation, harvested footage of students without consent and was criticized for being used by the Chinese government for surveillance. 

Similar cases of data extraction without consent occurred at Stanford University and Microsoft, where millions of photos were scraped for machine learning research.



The MS-Celeb dataset contained millions of photos of individuals, including activists and critics of facial recognition, without their consent. 

Even anonymized datasets can lead to re-identification and reveal sensitive personal information, such as the New York City taxi dataset being used to infer taxi drivers' religious beliefs and strip club visits. 

Collecting large datasets has become a norm in the AI field for success in machine learning, despite ethical, political, and privacy concerns.

The article discusses the normalization of data extraction and the myths surrounding its value and necessity. The belief that more data is always better has been perpetuated by vested interests, institutions, and technology, leading to a moral imperative to collect as much data as possible. However, this approach raises ethical and epistemological concerns and potential harms.

The article discusses the idea of data as a resource and the shift in its perception from something personal to something inert and non-human. The belief that data is a limitless resource to be collected and refined like oil raises ethical concerns and disguises the material origins and consequences of data extraction. The metaphor of data as a natural resource is a rhetorical trick used for centuries by colonial powers to justify extraction. The view of data as capital aligns with neoliberal visions of markets as the primary forms of organization.

Data functions as a form of capital, driving a cycle of perpetual data extraction in the name of capital accumulation. High achievers in the mainstream economy benefit while the poorest are subject to harmful forms of data surveillance. The ideology of data promotes mass extraction, which is foundational to the functioning of AI. The ongoing flow of data is necessary to improve machine learning models, which perpetuates the justification for more data extraction. Those who benefit from data extraction seek to maintain the status quo.

The emergence of data subjects has shifted the focus away from human subjects and their rights. University-based AI research has historically been exempt from ethical review processes, as it was seen as low-risk applied math. However, the use of machine learning in sensitive domains like education and healthcare has led to potential harms to individuals and communities. The presumption that publicly available datasets pose minimal risks and should be exempt from ethics review persists.

The assumptions underlying AI risk are outdated and no longer applicable due to easier data connectivity, indefinite repurposing, continuous updating, and data removal from its collection context. AI tools are becoming more invasive, and researchers are accessing data without interacting with their subjects. Researchers use databases like CalGang, known for its inaccuracies, as definitive sources for training predictive AI systems. This can lead to disproportionate inclusion of Black and Latinx people based on trivial justifications, perpetuating biases.

The separation of ethical questions from technical questions in AI research reflects a wider problem in the field, where responsibility for harm is often not recognized or seen as beyond the scope of research. Researchers can unknowingly reproduce ideas that damage vulnerable communities and reinforce current injustices, perpetuating the false idea that scientific research happens in a vacuum. This is particularly dangerous as AI moves from being experimental to being tested at scale on millions of people, with harmful assumptions potentially being deployed in production systems.



 Machine learning and data science methods create a distance between researchers and individuals, making it hard to reverse harm. 

 The use of data has a long-established practice of operating at a psychological distance from the people reflected in datasets. 

 Joseph Weizenbaum suggested that scientists and technologists should think deeply about the consequences of their work and reduce psychological distances, but this standard did not become mainstream. 

 There is a culture of data harvesting that can be exploitative and invasive, producing lasting harm. 

 The current culture of data extraction continues to grow despite concerns.



 AI development involves researching thousands of datasets that capture biodata, including biometric, sociometric, and psychometric data. 

 These datasets raise ethical, methodological, and epistemological questions, as many were harvested without consent or knowledge. 

 The data is used to fuel predictive policing tools, expand facial recognition systems, modulate health insurance rates, and penalize distracted drivers. 

 Tech companies gather data from devices such as watches, phones, and tablets, and assess gestures and facial expressions in workplaces and classrooms. 

 The collection of people's data raises privacy concerns, such as the deal between Britain's Royal Free National Health Service Foundation Trust and Google's DeepMind to share patient data.



DeepMind violated data protection laws by not informing patients sufficiently. 

Data extraction and training dataset construction are a form of privatization of public goods. 

The value of public data should come back to the public good, but it is now privately held by a handful of companies. 

The AI field works on a powerful extractive logic that enriches tech companies while diminishing spaces free from data collection. 

Machines have enormous appetites, and what they are fed has an enormous impact.



Data gathering and labeling for AI is a social and political intervention that shapes how AI models interpret the world. 

Data classification and naming is an act of world-making and containment, with enormous ramifications for the communities affected. 

The myth of data collection as a benevolent practice in computer science obscures its operations of power and avoids responsibility for its consequences. 

The example of Samuel Morton's collection of human skulls demonstrates the historical roots of data classification and labeling.

The text discusses the history of craniometry, a method used to classify and rank human races by comparing the physical characteristics of skulls. The founder of craniometry, Samuel Morton, aimed to objectively classify races and rank them based on skull size. This practice was legitimized by white European and American scholars and used to justify racist violence and dispossession. Morton's tables of average skull volume by race were considered objective data and used to maintain the legitimacy of slavery and racial segregation in the United States. However, Stephen Jay Gould later discredited Morton's work, revealing that it was not the kind of evidence it claimed to be.

The text discusses Stephen Jay Gould's criticism of Samuel Morton's craniometry work, which was used to justify racial oppression in the United States. Gould found evidence of unconscious bias and errors in Morton's work, and contemporary assessments showed no significant differences among people. The use of craniometry was based on erroneous assumptions and was debunked as a race science, but its dominant metaphors, logics, and categories supported white supremacy and influenced political ideas about race.



The legacy of correlating cranial morphology with intelligence and claims to legal rights using skull measurements acts as a technical alibi for colonialism and slavery. The focus should not be on correcting errors in skull measurements, but condemning the approach altogether due to its invalid assumptions about intelligence, race, and biology. The practices of classification used by Morton were inherently political and had far-ranging social and economic effects. The politics of classification is a core practice in artificial intelligence, and companies tend to focus on addressing technical errors and skewed data rather than questioning how classification functions in machine learning and its interactions with the classified. Geoffrey Bowker and Susan Leigh's landmark study on classification can provide insight into these questions.



Classifications are powerful technologies that can become invisible yet still shape the social and material world. The focus on bias in artificial intelligence has detracted from assessing the core practices of classification in AI and their politics. The chapter explores how training datasets naturalize hierarchies and magnify inequalities, and how AI uses classification to encode power. Discriminatory AI systems are legion, from gender and age bias to racism and misogyny. The limitations of addressing bias with mathematical parity instead of contending with underlying social, political, and economic structures are also discussed.

The text discusses how incidents of AI bias and discrimination are often revealed by journalists or whistleblowers, leading companies to promise to address the issue and make technical interventions. However, the underlying issues causing bias are rarely debated publicly. An example of bias is given through Amazon's experiment with automating the hiring process, which led to the AI system valuing subtle cues over commonly used engineering terms.

Amazon's experiment with an AI system for hiring revealed biases against women due to gendered language and the past employment practices of the company. The AI industry has traditionally seen bias as a bug to be fixed rather than a feature of classification, leading to a focus on quantitative parity rather than understanding the relationship between bias and classification. The scale of the bias problem goes deeper than a single system or failed approach.

The article discusses the issue of bias in artificial intelligence (AI) systems, which arises from the biases in the data used to train them. The author argues that instead of focusing on debiasing the data, it is important to look at the societal inequalities that shape the data and the systems that produce it. The author discusses IBM's attempt to address bias in its facial recognition systems by creating a more inclusive dataset called Diversity in Faces (DiF), but also points out the limitations of such efforts.

The article discusses IBM's attempt to address bias in facial recognition systems through a more diverse dataset, but points out the limitations of this approach. The classifications used in creating the dataset reveal the politics of what diversity means, and the emphasis on cranial measures ultimately depoliticizes the idea of diversity. The article also criticizes the claim that aspects of identity such as race and gender can be observed from the face, as this ignores the socially constructed nature of identity. This is an example of what Simone Browne calls digital epidermalization, the imposition of race on the body through technical systems.

The use of surveillance technologies leads to alienation of individuals by producing a truth about their identity despite their claims. IBM's approach to diversity classification is centralized and driven by machine learning techniques, which prioritize the tools' affordances over cultural understanding. Technical claims of accuracy and performance are often grounded in an ideological premise of biology as destiny. The act of classification has historically been aligned with power, and bias as a term has evolved to mean undue prejudice.

In machine learning, bias refers to a systematic classification error that can occur during the predictive process of generalization. It is often contrasted with variance, which refers to an algorithm's sensitivity to differences in training data. Outside of machine learning, bias has other meanings, such as preconceived notions or opinions in law and cognitive biases in psychology.

The term bias in AI is limited in utility as it encompasses not just technical errors but also unconscious attitudes and stereotypes that lead to discrimination. The focus on statistical bias as a remedy for deeper structural problems overlooks the ways in which AI systems reflect and perpetuate structural inequality. Every dataset used to train machine learning systems contains a worldview and involves inherently political, cultural, and social choices. Understanding these classifications can reveal the various forms of power built into AI world-building. ImageNet is an example of a benchmark training set that has influenced computer vision research since its creation in 2009.

ImageNet's structure is based on WordNet, a database of word classifications developed in 1985 for computational linguistics and natural language processing. ImageNet's taxonomy is organized according to a nested hierarchy derived from WordNet, in which each synset represents a distinct concept, with synonyms grouped together. The classification system moves from more general concepts to more specific ones, evoking prior taxonomical ranks such as the Linnaean system of biological classification. The true strangeness of ImageNet's worldview is reflected in its nine top-level categories.

The text discusses the categories in ImageNet and how they are organized into specific nested classes. It includes examples of categories, such as apple varieties and hot objects. The classification of human bodies is also mentioned, highlighting an implicit assumption of recognizing only male and female bodies as natural. The article also notes the diverse range of image types and qualities found within the dataset.

The text highlights the problematic politics behind classifying gender in ImageNet and how it naturalizes gender as a binary and binary-based biological construct, rendering transgender and gender nonbinary people invisible or deviant. This binary-based approach is similar to harmful classification schemes used in the past, such as the classification of homosexuality as a mental disorder. The text argues that categorizing people without their input or consent has a long history of justifying violence and oppression, and that these classifying logics are not fixed but rather produce a looping effect that impacts the people being classified.

The classification of people can stabilize political categories that are difficult to resist, and influential infrastructures and training datasets in the AI field contain political interventions within their taxonomies. The concept of ascribing phenomena to a category reifies the existence of that category, but ImageNet's approach erases gradients and flattens everything to a label. ImageNet contains highly populated categories for people that reveal the outlines of a worldview, including race, age, and nationality.

ImageNet's taxonomy contains problematic categories for classifying people based on their race, nationality, profession, and even non-visual concepts such as relationships. Offensive categories related to misogyny, racism, ageism, and ableism are also present. The dataset reifies these categories and connects them to images for future recognition systems.

ImageNet, a dataset widely used for object recognition, contained offensive terms including racist slurs and moral judgments in its "Person" categories for 10 years. The ImageNet Roulette project in 2019 drew public attention to the issue, leading to the removal of 56% of the "Person" categories deemed unsafe. However, the authors continue to support the automated classification of people based on photographs despite notable problems, and the concept of "safe" classification remains subjective. The larger system of human classification is complex and potentially dangerous.

ImageNet's categories reflect assumptions and stereotypes about race, gender, age, and ability, even when the labels aren't offensive. The use of classification schemes in tech companies like Facebook is harder to investigate and criticize, and the images used in ImageNet's "Person" categories were harvested without people's knowledge and labeled by low-paid crowdworkers, leading to biases and stereotypes. The bedrock layer of labeled images is beset with errors and problems.

The article discusses the limitations and problems of AI training datasets, particularly ImageNet, in simplifying complex cultural materials and flattening social relations into quantifiable entities. It highlights the dangers of classifying people based on gender and race, as these categories are often falsely assumed to be natural and fixed biological categories. The approach of labeling data fails to contend with power dynamics of classification and precludes a thorough assessment of underlying logics. Overall, the article emphasizes the need to be critical of the worldview that underpins AI training datasets and to be aware of the assumptions and biases they contain.

The article discusses the problematic categorization of gender and race in AI training datasets, exemplified by the UTKFace dataset, which categorizes gender as binary and race as five classes. These categorizations are harmful and reflect the reductive racial classifications of the twentieth century, such as South Africa's apartheid system. The article highlights the political implications of such classifications and the need for a critical perspective on the assumptions and biases underlying AI training datasets.

Automated systems that classify and categorize people based on characteristics like race and gender reproduce systemic inequality and exacerbate it. The concept of a pure race signifier has always been in dispute, yet machine learning systems seek to classify relational things into fixed categories, which is scientifically and ethically problematic. These systems construct race and gender, defining the world within their terms, and causing long-lasting ramifications for people.

Automated systems that predict identities and future actions erase the political and normative interventions involved in categorization, creating a narrow range of recognizable identities and perpetuating the negative effects of historical classifications. Classifications are technologies that produce and limit ways of knowing and are built into the logics of AI, making it necessary to account for their power and politics when designing systems.

The article discusses the problem of AI systems perpetuating oppression and discrimination due to biased data inputs and calls for a new approach that considers the ethical implications of these systems. The authors argue that justice in AI systems cannot be solely achieved through optimization metrics and statistical parity, and requires an understanding of how AI systems interact with data, workers, the environment, and individuals affected by its use. They also call for sensitivity to the topography of classification schemes and the uneven allocation of advantage and suffering. The article emphasizes the need to change nonconsensual classification practices and address normative assumptions about identity.

The article explores the consequences of machine categorization in the context of machine learning and the lack of oversight in the global scale of private technology companies like Facebook, Google, TikTok, and Baidu. The authors argue that classification has a real impact and is not just a historical curiosity, and the most harmful forms of human categorization require political organizing, sustained protest, and public campaigning over many years to change. The article emphasizes the need for public contestation and meaningful avenues for opposition to the internal logics of machine learning classification.

The article discusses how the concept of universal emotions or affects was developed by psychologist Paul Ekman in Papua New Guinea in 1967 and how it has evolved into the expanding industry of affect recognition in artificial intelligence, worth over $17 billion. The article highlights the problems with affect recognition and the need for a political response to the hidden processes of AI that can cause advantage or disadvantage to people.

The article explores the history of affect recognition and how it has evolved into an expanding industry in artificial intelligence, despite shaky evidence that a person's interior state of feeling can be accurately assessed by analyzing their face. The article traces the development of these ideas, from U.S. intelligence funding during the Cold War to current AI-based emotion detection tools employed in everyday life, and highlights the ethical concerns and scientific doubts surrounding these methods. Paul Ekman's research in Papua New Guinea is discussed as one of many contributors to the theories behind affect recognition.

Automated affect recognition is a technology that aims to extract information about people's emotions from images, and it is being built into facial recognition platforms by both large tech companies and small start-ups. Despite a lack of scientific evidence that it works, these systems are already widely deployed, particularly in hiring, and they have the potential to influence behavior and shape social institutions. Emotion recognition is a lucrative business, but it also raises concerns about privacy, bias, and the potential for abuse.

Companies like HireVue, Affectiva, Apple, Microsoft, and IBM are using machine learning to build emotion recognition systems that analyze facial cues and voice tone to infer people's emotional states. These systems are being used in various applications, such as assessing job candidates, measuring consumer emotional responses to advertising, detecting distracted drivers, and analyzing student engagement. The development of emotion recognition raises concerns about privacy, bias, and the potential for misuse.

Emotion recognition systems, such as those used by Amazon's Rekognition tool and API, are based on the assumption that there are a small number of distinct and universally expressed emotional categories, which can be detected by machines. This approach stems from the work of psychologist Paul Ekman, who was influenced by Silvan Tomkins' theory that affect is an innate and universal set of evolutionary responses. However, the taxonomization of emotions raises questions about the universality of emotional categories.

Psychologist Silvan Tomkins' theory of biologically based universal affects was developed as a challenge to behaviorism and psychoanalysis, which he believed reduced human consciousness to a mere by-product. Tomkins argued that affects, rather than drives, are the primary system governing human motivation and behavior, and that they can be universally recognized across cultures. Later applications of affect theory stress the inability of humans to recognize both the feeling and the expression of affects, which has led to the development of AI systems for emotion recognition.

The complexity of human motivation and affective feelings makes it difficult to identify causes and manage them. Affects are not as simple as drives, as they are not strictly instrumental and have a high degree of independence from stimuli and objects. Facial expression is a primary way to understand affects, as it is innately related to an organ system which is extraordinarily visible. Tomkins assumes that facial display of affects is a human universal.

Tomkins believed that affects are sets of muscle, vascular, and glandular responses located in the face and body, triggered at subcortical centers where specific programs for each distinct affect are stored. The interpretation of affective displays depends on individual, social, and cultural factors, leading to different dialects of facial language in different societies. The potential conflict between culturally variable facial expressions and a biologically based, universal language had implications for the study of emotion recognition. In 1965, Ekman was pushed to do affect research by the Advanced Research Projects Agency (ARPA), which was interested in understanding cross-cultural nonverbal communication.

Ekman dropped the pursuit of ARPA funding for his research due to his lack of expertise. However, Hough insisted and wrote the proposal that earned Ekman a million dollars in funding. Hough was eager to distribute the money quickly to avoid suspicion from Senator Frank Church. With this funding, Ekman began his studies on the universality of facial expressions, largely duplicating Tomkins's methods and using photographs to test subjects from various countries. His studies relied on asking participants to simulate emotions, which were then compared to expressions gathered outside of laboratory conditions. ARPA's funding was the first of many from defense, intelligence, and law enforcement agencies that supported Ekman's career and the field of affect recognition.

Ekman's methodology for studying the universality of facial expressions had flaws, such as the forced choice response format, posing of emotions, and potential cultural learning. To prove his theory, Ekman traveled to Papua New Guinea to study indigenous people's reactions to his categorized posed expressions. He later devised an alternative approach, asking US research subjects to choose from six affect concepts based on photographs, and found close enough results to claim universality. The idea of inferring internal states from external signs comes from physiognomy's history of studying a person's expressions.

Physiognomy, the practice of judging a person's character based on their physical appearance, was used in ancient Greece and reached its peak during the 18th and 19th centuries in Western culture. Swiss pastor Johann Kaspar Lavater blended physiognomy with the latest scientific knowledge and used silhouettes to create a more objective comparison of faces. Skull measurement was used to support emerging nationalism, racism, and xenophobia, and this practice continued through the work of phrenologists and scientific criminologists in the 19th century. These types of inferential classifications are seen in contemporary AI systems.

French neurologist Duchenne codified the use of photography and electrical shocks in the study of human faces, connecting older ideas from physiognomy and phrenology with more modern investigations into physiology and psychology. He aimed to build a more complete anatomical and physiological understanding of the face and used collodion processing to freeze fleeting muscular movements and facial expressions in images. Duchenne believed that the use of photography and other technical systems would transform representation into something objective and evidentiary, suitable for scientific study.

The text discusses the use of photography in capturing and categorizing human emotions, particularly in the works of G.-B. Duchenne and Paul Ekman. Ekman used slow motion photography to identify microexpressions, which he later codified into a system called the Facial Action Scoring Technique (FAST) but faced issues when others produced facial expressions not included in its typology. He then identified roughly forty distinct muscular contractions on the face and called them Action Units.

Paul Ekman and Wallace Friesen published the Facial Action Coding System (FACS) in 1978, which was very labor-intensive to use as a measurement tool. Ekman heard of a solution to automate measurement at a conference in the early 1980s, likely referring to Igor Aleksander's early machine learning object-recognition system, WISARD. WISARD was trained on a database of known football hooligans and was likely a common cause with Ekman's approach to analyzing faces. Ekman claims to have played an active role in driving automated forms of affect recognition and helped set up an informal competition between two teams working with FACS data, which had lasting impact on the affective computing field.

The article discusses the history of emotion recognition technology, highlighting the contributions of psychologist Paul Ekman and two research teams led by Jeffrey Cohn and Takeo Kanade. Ekman's Facial Action Coding System (FACS) provided a set of labels and measurements for categorizing facial expressions, which was crucial for later machine learning applications. The Cohn-Kanade emotional expression dataset was developed for affect recognition research, based on Ekman's tradition of posed facial expressions. The article also mentions the need for standardized image databases for facial research, which led to the creation of the FERET program and the assembly of labeled datasets for machine learning research.

The article discusses the evolution of photo databases used for affect recognition research. Researchers created standardized image databases, such as Karolinska Directed Emotional Faces and the Cohn-Kanade dataset, to train machine learning systems to recognize facial expressions. The use of spontaneous facial expressions in real-life settings also became popular, with companies like Affectiva collecting data through webcam recordings. However, the article notes a problem with the use of extreme posed expressions and the potential for bias in hand-labeled data.



FACS, developed by Ekman from posed photographs, had a wide-ranging influence on lie detection software, computer vision, and popular culture. 

His techniques of deception detection were sold to security agencies, including the SPOT program, which has been criticized for racial profiling and lack of scientific methodology.



Critiques of Ekman's theories emerged as his fame grew, with skepticism about the universality of emotions and oversimplification of emotions into binary categories. 

Scientists from various fields, including psychology and social science, have raised concerns about the reliability of facial expressions as an indicator of internal mental states, particularly in the context of AI. 

There is no consensus among researchers about what an emotion actually is in the field of the study of emotions.

The concept of emotions and their definition, formulation, expression, and physiological functions remain unresolved. Criticisms of Ekman's theory of emotions include the circularity in his method and the assumption that facial expressions are universally recognized and free from cultural influence. Technical systems that use simulated emotional states to train AI systems raise further problems.

There is debate about whether emotions can be accurately grouped into a small number of discrete categories, and evidence is lacking for the assumption that each kind of emotion can be identified by a unique physical response. Facial expressions may also not indicate true interior states. Despite controversy surrounding Ekman's theory of emotions, it remains prominent in AI applications and cited by many papers. Some researchers question the idea of computers sensing emotions, and evidence against Ekman's work continues to grow.

A review of literature on inferring emotions from facial expressions found that facial expressions are not reliable indicators of emotional states across cultures and contexts. Technology companies claiming to automate inference of emotions are criticized, and it is noted that there is little known about how and why certain facial movements express emotion. The approach of reading emotions from the face has endured due to military research funding, policing priorities, and profit motives shaping the field since the 1960s.

Ekmans theories and methodologies are cited as settled in the AI field, but fail to address more complex issues of context, conditioning, relationality, and cultural factors. The focus has been on increasing accuracy rates of AI systems, rather than questioning the origins and social/political consequences of emotional categories. Affect recognition tools are being deployed in political attacks, highlighting the need for a critical approach to the politics of facial expressions.

A blogger used various facial and speech analysis tools to assess videos of Congresswoman Ilhan Omar, claiming that she had a high lie score and was registering high on stress, contempt, and nervousness. Conservative media outlets ran with the story, but the reliability of these affect recognition tools is questionable due to racial biases and the complexity of emotions. Emotion detection systems do not directly measure people's mental states, but rather statistically optimize correlations of physical characteristics among facial images. The scientific foundations of automated emotion detection are in question, yet a new generation of affect tools is emerging.

Despite evidence of the unreliability of affect detection, companies continue to seek new sources for facial imagery to mine for profits in a growing range of high-stakes contexts, from policing to hiring. Attempts to infer internal states from facial movements alone are incomplete and lack validity. If we continue to automate affect recognition, it risks unfair judgment of job applicants, students, and customers based on questionable methodologies. These systems rely on a narrow understanding of emotions, limiting our ability to capture the complexities of human feeling and expression.

The text discusses the oversimplification of complex human experiences by AI systems and the limitations of technology in capturing emotional nuances. The author describes their experience researching the Snowden archive, which contains classified documents on the use of machine learning in the intelligence sector. The archive holds a wealth of information that was strictly off-limits to those without high-level clearance and captures the years when data collection expanded to include phones, browsers, social media, and email as sources of data for the state.

The text reveals that the intelligence community contributed to the development of artificial intelligence techniques, and the Snowden archive exposes a parallel AI sector developed in secrecy. The methods share similarities but differ in terms of reach, objectives, and results. The archive shows programs such as TREASUREMAP and FOXACID that track and record internet activity and target individuals to own them. These programs resemble the work of social network mapping and manipulation companies like Cambridge Analytica and Amazon Ring. The documents released in 2013 still read like current AI marketing brochures.

The US intelligence agencies and military have been major drivers of AI research since the 1950s, with military support and priorities strongly guiding the field. The NSA is working on aggressively pursuing legal authorities and changing policy frameworks to fit the information age. The NSA deploys targeted emails that require guilty knowledge about the target and the agency reports everything they do back to base. The goal is to change laws to fit tools, not the other way around.

AI research funded by the US military and intelligence agencies has profoundly shaped the field and infused it with classificatory thinking, situational awareness, and targeting. Big data and machine learning expanded the modes of information extraction and informed a social theory of tracking and understanding people through metadata. State and corporate actors collaborate to produce infrastructural warfare, and technologies developed for intelligence agencies have filtered down to government and law enforcement agencies. The relationship between national militaries and the AI industry has expanded beyond security contexts.

The commercial surveillance sector is increasingly marketing its tools to police departments and public agencies, and algorithmic governance is reshaping the traditional role of states. AI operates within a complex, multinational network of tools and infrastructures. China has stated its commitment to be the global leader in AI.

The language of war is being used to assert sovereign power over AI and to redraw the power of tech companies within the bounds of the nation-state, with geopolitical competition increasingly blurring the lines between commercial and military sectors. In the United States, the Third Offset strategy was developed to seek national control and international dominance of AI for military and corporate advantage.

The concept of using technology as a force multiplier to balance military capabilities has been around since the 1950s. The Third Offset strategy proposed by the US Department of Defense in 2014 aims to use AI, computational warfare, and robots. However, the military lacks the necessary AI resources, expertise, and infrastructure, which requires partnering with the tech industry. While the NSA has already worked with tech companies to exploit their advances, political pushback and limitations on access to real-time data have emerged. Silicon Valley's expertise in AI infrastructures is needed to create the necessary infrastructure for warfare, but convincing the tech sector without alienating their employees and the public remains a challenge.

In 2017, the US Department of Defense announced Project Maven, which aimed to integrate AI and machine learning to improve military operations. The project was a small piece of the larger Joint Enterprise Defense Infrastructure cloud project, which aimed to redesign the entire IT infrastructure of the Defense Department. Project Maven focused on creating an AI system that would allow analysts to select a target and track enemy combatants using drone footage. The necessary technical platforms and machine learning skills were found in the commercial tech sector, and companies such as Amazon, Microsoft, and Google competed for the contract. Google won the first contract, which involved analyzing military data collected from drones in areas where US privacy laws did not apply.

Google's Project Maven involved using its TensorFlow AI infrastructure to detect objects and individuals in drone footage for military purposes. However, after over 3,100 Google employees protested against the project, citing that Google should not be involved in warfare, the company withdrew from the competition for the Pentagon's JEDI contract. The contract ultimately went to Microsoft, which outbid Amazon. The incident led Google to release its AI principles, which include a section on not pursuing the development of technologies whose primary purpose is to cause injury to people.

The debate over the use of AI in warfare shifted from a focus on whether to use it at all to questions of precision and technical accuracy, which obscured the deeper problems with automated warfare. The use of object detection raises questions about the building of training sets and the classification of threats, which are inherently unstable and political. The Maven episode highlights the deep schisms in the AI industry regarding the military-civilian relationship, and the AI war instills fear and promotes unquestioning support for a nationalist agenda. Google's pursuit of higher security certifications to work more closely with the Defense Department reflects its commitment to supporting a nationalist agenda.

Tech companies are increasingly aligning with national interests, with AI technology being outsourced to municipal-level services and institutions. Palantir, a company established by PayPal billionaire Peter Thiel, provides militarized forms of pattern detection and threat assessment to anyone willing to pay for an intelligence advantage. Thiel argues that AI is primarily a military technology and the tools are valuable for gaining an intelligence advantage. Palantir's CEO describes the company as patriotic.

Palantir, a Silicon Valley start-up, initially worked with federal military and intelligence agencies and later expanded to work with hedge funds, banks, and corporations. Its business model includes data analysis and pattern detection using machine learning, and it also provides outsourced surveillance services. Palantir's DNA was shaped by working for and within the defense community, and it designed databases and management software to drive the mechanics of deportation for ICE. Palantir's CEO has an early intellectual interest in aggression and believes that the desire to commit violence is a constant founding fact of human life.

Palantir, a global intelligence platform designed for the War on Terror, is now being used against civilians in the US. Its tools are built to collect everything and look for anomalies in the data, which has been directed against ordinary Americans at home. Palantir has been used by the U.S. Department of Health and Human Services, FBI, Department of Homeland Security, and ICE for various purposes, including detecting Medicare fraud, criminal probes, screening air travelers, keeping tabs on immigrants, and deportation. Palantir's phone app, Falcon, functions as a vast dragnet and has been used by ICE agents to guide their raids. Despite Palantir's secrecy, its patent applications give some insight into the company's approach to AI for deportation.

Palantir's facial recognition and back-end processing system allows them to photograph people in short-time-frame encounters and run their image against all available databases, creating a framework for arrests or deportations. This represents a shift towards military intelligence infrastructures, which could lead to widespread surveillance with very little oversight. Sociologist Sarah Brayne observed that Palantir's data platforms are turning police into intelligence agents, transforming the process of surveillance entirely. The shift from traditional to big data surveillance is associated with a migration of law enforcement operations toward intelligence activities, which is fundamentally predictive.

Palantir's surveillance software reproduces inequality by subjecting predominantly poor, Black, and Latinx neighborhoods to even greater surveillance. The point system creates a reinforcing loop of logic, leading to a feedback loop where individuals in the criminal justice system are more likely to be surveilled and further drawn into the surveillance net. The machine learning approaches of Palantir and similar systems intensify the problems of overpolicing and racially biased surveillance, exacerbating historical inequality.

Private vendors of AI technologies like Palantir should be legally accountable for the harms produced when governments use their systems. Currently, vendors and contractors have little incentive to ensure that their systems aren't reinforcing historical harms or creating new ones. One example is Vigilant Solutions, which takes surveillance tools that would require judicial oversight if operated by governments and turns them into a private enterprise outside constitutional privacy limits. Vigilant's automatic license-plate recognition cameras photograph every passing vehicle and create a massive database of people's movements.

Vigilant, a California-based company, collects and stores license plate images in a perpetual database and sells access to it to various entities, including police, banks, and insurance companies. It partners with governments to provide police with Automatic License Plate Reader (ALPR) systems and in return, receives records of outstanding arrest warrants and overdue court fees. It also signed a contract with ICE, giving the agency access to license plate records gathered by private businesses and law enforcement agencies across the US. Vigilant charges a 25% surcharge and keeps records of every license plate reading to add to its database.

Vigilant, a private company, collects and sells access to license plate and facial recognition databases to various entities, blurring the line between private contractors and state entities. Social media crime-reporting apps like Neighbors allow users to receive real-time alerts, discuss incidents, and share security camera footage, often with police. Amazon's Ring doorbell cameras help build large-scale training datasets, with classificatory logics of normal and anomalous behavior aligned with battlefield logics of allies and enemies. However, such residential surveillance ecosystems erode the differences between public and private sector data practices, and often feature racist commentary.

Amazon's Ring system is being used for public and private surveillance, with police departments being given discounts and access to homeowner footage without a warrant. The Neighbors app incentivizes people to download and use Ring, creating a self-perpetuating surveillance network. The system disproportionately targets people of color and creates a new layer of worker surveillance for Amazon employees. These targeting practices, from advertising to drone surveillance, should be considered as one interconnected system of power.

Intelligence agencies have dispersed their targeting practices throughout social systems with the help of technology companies. Signature strikes, based on a person's metadata signature, allow for people to be killed without their identity being known. Drone strikes are not as precise as they are sold to be, and are based on correlation rather than precision.

The use of pattern recognition without definitive proof is common in many domains and often takes the form of a score. In 2015, during the Syrian refugee crisis, IBM was approached to use its machine learning platform to automatically distinguish terrorists from refugees by detecting a data signature. IBM created an experimental terrorist credit score to identify ISIS fighters among refugees based on assumptions and social media analysis.

Data analysts harvested unstructured data from various sources to develop a hypothetical threat score based on past addresses, workplaces, and social connections of individuals. These systems of state control, influenced by socially constructed models of creditworthiness, are used to reward predetermined social behavior and penalize non-conformity. Credit scoring has become a place where military and commercial signatures combine and is deeply entwined in law enforcement, border control, and access to public benefits. These AI systems are primarily used to surveil, assess, and restrict disadvantaged populations.

The former Republican governor of Michigan, Rick Snyder, implemented two algorithm-driven austerity programs to disqualify individuals from food assistance and unemployment benefits, leading to the inaccurate identification and disqualification of thousands of Michigan residents. The systems were financial failures and caused harm to many individuals, leading to successful lawsuits against the state. The targeting of certain groups, such as fugitive felons and suspected fraudsters, reflects the consistent logics seen in state-driven AI systems.

The use of militaristic command-and-control systems for punishment and exclusion undermines the social and economic stability that they aim to promote. State bureaucracy structures and automated decision systems perpetuate a threat-targeting model that evaluates and scores individuals and communities. The idea of data agriculture, in which data is extracted globally to maintain US hegemony, is a flawed philosophy that breaks down under scrutiny due to the complex and messy nature of planetary computation that exceeds national designs. The armature of planetary-scale computation has a self-reinforcing logic that is used on behalf of national designs but also exceeds them.

The notion of AI being securely contained within national borders is a myth as AI infrastructure and labor force supporting it are already hybrid. AI and algorithmic systems used by the state reveal a philosophy of en masse infrastructural command and control through extractive data techniques and surveillance, with deep intermingling of state, municipal, and corporate logics. However, this results in an uncomfortable bargain as technology companies are taking on state functions they are ill-suited to fulfill and might be held liable in the future. The Snowden archive reveals the overlapping and contradictory logics of surveillance that have extended to many state functions. All the money and resources spent on relentless surveillance is part of a fever dream of centralized control that has come at the cost of other visions of social.

The Snowden disclosures revealed a culture of extraction where the state and commercial sector collaborated, and NSA methods and tools have filtered down to various sectors of everyday life. AI systems are not objective, universal, or neutral but embedded in social, political, cultural, and economic worlds shaped by institutions and imperatives that determine what they do and how they do it. They are designed to discriminate, amplify hierarchies, and encode narrow classifications, reproducing and amplifying existing structural inequalities. AI systems are expressions of power that benefit states, institutions, and corporations, emerging from wider economic and political forces.

The standard narrative of AI centers on algorithmic exceptionalism that portrays AI systems as smarter and more objective than humans. Games have been a preferred testing ground for AI programs since the 1950s. AIs history is filled with narratives of magic and mystification that draw attention to spectacular displays of speed, efficiency, and computational reasoning. However, this narrative overlooks the fact that AI is primarily a product of human labor and capital investment.

Enchanted determinism is a central logic of machine learning that sees AI systems as enchanted, beyond the known world, yet deterministic. It gives rise to a faith that mathematical formalisms will help us understand humans and society, and an epistemological flattening of complexity into clean signal for the purposes of prediction. Enchanted determinism obscures power, closes off informed public discussion, and has two dominant strands: a form of tech utopianism that offers computational interventions as universal solutions and a tech dystopian perspective that blames algorithms for negative outcomes without contending with their underlying social and political context.

The tech dystopian and utopian narratives offer an ahistorical view that places technology at the center, ignoring the systemic forces that it magnifies and serves. The ideology of Cartesian dualism persists in AI, with the fantasy that AI systems are disembodied brains that operate independently from their creators. However, AI's surprising moves in games are a result of statistical analysis at scale, not preternatural intelligence.

The expansion of the AI industry has been publicly subsidized, from defense funding and federal research agencies to public utilities and tax breaks. The blueprint of Google's data center reveals how much of the technical vision depends on public utilities, and the engineering plan emphasizes the political economies of its construction. The definition and boundaries of AI are important, and the illusions of AI distract from the relevant questions of whom these systems serve and what are the wider planetary consequences.

The book explores the extractive industry and political economy of AI, which is shaped by the entanglement of technology, capital, and power. The industry narratives of AI as an abstract computational cloud abstract away the material conditions of production while extracting more information and resources. The core issue is the deep entanglement of technology, capital, and power. To see the full life cycle of AI and the dynamics of power that drive it, one needs to go beyond the conventional maps of AI and locate it in a wider landscape. The real stakes of AI are the global interconnected systems.

AI is born from extractive politics that rely on rare earth minerals, water, coal, and oil. The labor force of AI is far greater than we normally imagine, with workers ranging from miners in Indonesia to crowdworkers in India and contract laborers in tech companies. AI is used to navigate drones over Yemen, direct immigration police in the United States, and modulate credit scores of human value and risk across the world. To understand AI for what it is, we need a wide-angle, multiscalar perspective on the overlapping regimes of extraction and power that it serves.

AI systems require human support to function seamlessly, such as tagging, correcting, evaluating, and editing. They also rely on human labor for tasks that are cheaper and easier to perform than using robots. The use of workplace AI results in power imbalances, with employers having more control over workers. AI systems exploit differences in time and wages to speed up the circuits of capital. The harvesting of the real world has intensified to capture spaces that were previously hard to reach, such as public spaces and social media feeds.

Personal data from language and photo-sharing sites is commonly used to train machine vision and language algorithms, with little questioning in the AI field. However, the process of categorizing and labeling data for AI training is inherently political and carries inherent biases. The practice has resulted in the creation of operational images, which are representations of the world solely for machines. Bias is a symptom of a normative logic used to determine how the world should be seen and evaluated, with affect detection drawing on controversial ideas about the relation of faces to emotions. Institutions have historically classified people into identity categories, perpetuating normative and stereotypical analyses of character and merit.

The article discusses the epistemological violence and reductionism involved in the use of machine learning to systematize and formalize human emotions, preferences, and identifications into machine-readable data. This reductionism involves ignoring or obscuring countless features and transmuting difference into computable sameness. The article highlights the danger of essentializing and imposing identities based on external appearances in AI systems, particularly in the context of state power.

The article discusses the interconnections between the military, the tech sector, and the government in the deployment of AI systems for surveillance and warfare, resulting in a profound expansion of surveillance and a blurring of private and public spheres. The article highlights the use of logistic systems of total information control for undocumented immigrants, welfare decision-making, and license plate readers. The article suggests the need for connected movements for justice to democratize AI and prevent its misuse for oppressive purposes.

The idea of democratizing AI for justice and equality is problematic as AI infrastructures and power structures are centralized and skewed towards control. AI ethics principles, while numerous, are often produced by economically developed countries with little representation from the regions most harmed by AI systems. Moreover, ethical principles lack enforceable and accountable means of implementation, allowing tech companies to choose how to deploy technologies and decide what ethical AI means for the rest of the world.

Ethics alone cannot address the fundamental concerns of AI, as public companies prioritize profits over ethics. To counter AI's amplification and reproduction of power, focus should be shifted towards centering the interests of affected communities. This requires assessing the labor conditions, treatment of immigrants, and environmental impact of AI systems. The long-term consequences of AI must be considered and a society committed to making artificial realities with no regard for harm must be avoided. The social contract for technology must be re-evaluated and the focus shifted to justice across all AI systems.

The transformations brought about by technology, including climate crisis, wealth inequality, racial discrimination, and labor exploitation, were not unforeseeable as they were always based on corporate logic. A new critique of technology is necessary to address issues of social, economic, and climate injustice and to find spaces beyond technical life. A renewed politics of refusal is needed to oppose the narratives of technological inevitability and to question why artificial intelligence ought to be applied rather than where it can be applied.

Donna Haraway's concept of the "informatics of domination" seeks to subject everything to statistical prediction and profit accumulation. Refusal of technology-first approaches requires rejecting the idea that the same tools that serve capital, militaries, and police are also fit to transform schools, hospitals, cities, and ecologies. National and international movements are needed to address underlying inequities and injustices, which are at their most powerful when they are united. Justice movements that address the interrelatedness of capitalism, computation, and control offer the greatest hope in challenging the structures of power that AI currently reinforces and creating the foundations for a different society. A different politics of inhabiting the Earth, repairing, and sharing the planet is necessary, and there are commons worth preserving.

The article discusses Jeff Bezos' passion for space and his private aerospace company, Blue Origin, which aims to shuttle astronauts and cargo to the Moon by 2024 and eventually establish a future where millions live and work in space. Bezos argues that moving out into space is necessary to avoid capping population and energy usage per capita on Earth. The article raises the question of whether such space exploration is necessary, and suggests that we should also consider finding ways to live beyond discrimination and brutal modes of optimization.

The article discusses the growing interest of tech billionaires, such as Jeff Bezos, Peter Diamandis, and Elon Musk, in space exploration and colonization. They envision a future where heavy industry is moved off-planet to giant space colonies, leaving Earth for residential building and light industry. Bezos is particularly concerned about the planet's limited supply of energy and the prospect of stasis. The article also highlights the ideology behind these space spectacles and their interconnectedness with the ideology of capitalism.

The extreme wealth and power of technology companies have enabled a small group of men to pursue their own private space race, often relying on government funding and tax incentives. Their aim is to extend extraction and growth across the solar system, driven by an imaginary of space, endless growth, and immortality. Bezos' inspiration for Blue Origin comes from Gerard K. O'Neill's fantasy of space colonization, which is inspired by the dismay and shock of the 1972 Club of Rome report, The Limits to Growth. The report predicted the end of nonrenewable resources and the impact on population growth, sustainability, and humanity's future on Earth.

The Club of Rome's report, The Limits to Growth, suggested that sustainable management and narrowing the gap between rich and poor nations were key to long-term stability of global society. However, it did not foresee the larger set of interconnected systems that make up the global economy, leading to greater environmental harms and resource depletion. Gerard K. O'Neill's book, The High Frontier, suggested space as a solution to the no-growth model by redirecting global anxiety over resource shortages with visions of serene space structures. However, the tech billionaires' visions of space colonization and frontier mining underscore a troubling relationship with Earth, as they seek to displace Earth's population and capture territory for mineral extraction.

The Commercial Space Launch Competitiveness Act, enacted in 2015, allows commercial space companies to own and profit from mining resources extracted from asteroids until 2023, despite the 1967 Outer Space Treaty recognizing space as a common interest. This legislation undermines the concept of space as a shared resource and fosters an imperialistic desire for space colonization, driven by the fear of death and the belief in unlimited technological progress.

The article discusses the history of space exploration, including the use of V-2 rockets developed by war criminal Wernher von Braun, and Jeff Bezos' admiration for him. The author visits Bezos' ranch in Texas, which has a violent colonial history, and reflects on the commercialization of space and the fear that drives it.

The article describes a visit to the Blue Origin suborbital launch facility in West Texas, which is a private infrastructure-in-progress guarded and gated. The author takes a photograph of the facility and is followed by two black Chevrolet pickups on the way to the town of Marfa.

The author expresses gratitude to the many individuals who contributed to the creation of the book "Atlas of AI" over several years, including scholars, friends, colleagues, and research communities. They mention specific people and groups, including Microsoft Research's FATE group and Social Media Collective, who taught them a great deal about the topic of AI.

The author expresses gratitude to various scholars and individuals who have contributed to the creation of the AI Now Institute at NYU and to the writing of their book, including research assistants, archivists, and editors. The author also acknowledges the institutions that provided them with time to write.

The author expresses gratitude to various institutions and communities where they developed the ideas in their book, including cole Normale Suprieure in Paris, the Robert Bosch Academy in Berlin, and the University of Melbourne. The author also acknowledges the feedback received from audiences at various conferences and the contribution of coauthors and journals in previously published material.

The text lists various academic publications on the social and ethical implications of artificial intelligence and machine learning, including topics such as facial recognition technology, bias in algorithms, worker surveillance, and the ethics of big data research. The authors discuss the potential harm that can arise from unregulated AI systems and call for greater accountability and transparency in their development and use.

The text mentions various academic publications and reports that the author has contributed to, in collaboration with various individuals and organizations, including the AI Now Institute. These publications address issues related to big data, privacy, and the social and ethical implications of artificial intelligence and machine learning. The author also acknowledges the support of individuals who have played a crucial role in the creation of the book.

The text includes notes on various works related to artificial intelligence (AI) and its limitations. It references the story of Clever Hans and critiques the idea of AI being able to replicate human intelligence. It also discusses the culture of exclusion in the tech industry and the negative impact of AI on society. The notes reference works by various authors, including Val Plumwood, Alan Turing, and John von Neumann, among others.

The notes reference various works and authors related to artificial intelligence, data colonization, and the impact of technology on society. It also acknowledges research communities that have informed the work and discusses the history and changes in San Francisco. The notes also reference works related to displacement and resistance, geological surveys, and the impact of surveillance capitalism.

The text includes a description of the geological variety of the Silver Peak area in 1909, along with references to Tesla's battery production, the extractive operations of the AI industry, and the risks associated with mineral commodity supply chains, including conflict minerals. It also references sources discussing responsible minerals policy and due diligence in supply chains, and the invisible networks of rare metals traders in global electronics supply chains.

The text discusses the complex network of traders, processors, and manufacturers involved in the supply chain of rare metals used in electronic devices, highlighting the ethical and environmental concerns associated with the mining and processing of these metals. It also addresses the energy consumption and carbon emissions associated with data centers and the use of artificial intelligence, and references sources discussing the need for greater energy efficiency practices and policy considerations in the field of AI.

The text discusses various topics related to environmental sustainability, materiality, transportation, and labor. It mentions Google's commitment to carbon neutrality, Microsoft's goal to be carbon negative by 2030, and the impact of shipping pollution on health and the environment. The concept of materiality is explored, and the loss of specialized labor due to the rise of decision-making systems is mentioned.

The text covers a range of topics related to labor and automation, including the historical development of industrial capitalism, the role of workers as appendages in factory systems, and the use of AI in digital labor platforms. It also explores issues of surveillance, responsibility, and control in these contexts, and includes references to relevant works by Marx, Foucault, and others.

The text consists of a list of numbered references covering various topics related to labor, technology, and power, including the challenges of establishing reliable supply chains, alternative work arrangements, worker surveillance, and time synchronization in technology. It also touches on the idea of power dynamics in institutions and the importance of organizing in the tech industry.

The text covers various topics, including labor issues and the use of biometrics and machine learning in technology. It cites conversations with labor organizers, tech workers, and researchers on these subjects. Additionally, it references the National Institute of Standards and Technology's work on the FBI's Automated Fingerprint Identification System, as well as various machine learning competitions and algorithms.

The text consists of a list of references and citations related to various topics such as the history of computing, artificial intelligence, dataset creation and use, facial recognition, and privacy concerns. It includes sources discussing the role of women in computing, the Enron corpus, dataset documentation and reporting, image datasets, and controversial uses of facial recognition technology. Some of the sources highlight efforts to address issues such as dataset bias and privacy violations.

The text discusses the deletion of various databases by Microsoft, Duke, and Stanford, as well as the potential harms of data collection and use. It references various sources that highlight the need for responsible data handling and caution against the use of data metaphors, such as "data is the new oil," which may obscure the real harms of data colonialism. The text also touches on the historical and contemporary practices of colonialism and their relationship to data collection and use.

The text includes references to various topics related to data and its impact on society. These include the concept of "ubercapital" as a form of capital, concerns around human subjects in big data research, the use of predictive policing technologies, and the history of race and racism in America. The text also references sources on design justice, data feminism, and failures to comply with data regulations.

The text discusses various issues related to bias in technology, including gender and racial biases in AI and machine learning systems. It references cases of bias in hiring tools and facial recognition systems. The text also includes sources on the history of inequality and discrimination in computation and the need for diversity in data and algorithms. Additionally, it discusses the concept of statistical bias in machine learning and legal cases related to technology and bias.

The text consists of a list of references and sources related to research into the biases and categories used in artificial intelligence, including ImageNet, and the need for more diverse and fair datasets. It includes references to studies on judgment under uncertainty, implicit bias, and the exclusion of certain categories from ImageNet. It also discusses the creation of the ImageNet Roulette app, which allowed users to see how ImageNet labels different images, including problematic and biased labels. The text references several datasets and studies focused on the importance of fairness and diversity in AI training sets.

The text includes numerous citations and references on topics related to artificial intelligence and its impact on society. Some key points include: - References to specific court cases and academic research on judgment under uncertainty and implicit bias - Discussion of the ImageNet dataset and the ImageNet Roulette app that highlighted problematic labels generated by AI - Examination of issues related to fairness and bias in AI datasets, particularly in facial recognition technology - Discussion of emotional recognition and its use in the workplace, including concerns about privacy and accuracy - References to various books and articles related to disability, identity, and categorization

The text references various works on emotional expression, affect imagery, and facial displays of emotion, including those by Barrett, Sedgwick, Tomkins, Leys, Darwin, Duchenne, Ekman, Rosenberg, McCarter, and Russell. It discusses the dissociation between feeling and cognition and its relevance to humanities theorists, as well as the universality of facial expressions of emotion across cultures. The text also mentions Lavater's Physiognomy and its significant historical influence.

This text is a list of references and citations related to the study of facial expressions and emotions, including works by Duchenne, Darwin, Ekman, and various researchers in the field of computer recognition of facial expressions. The references cover topics such as the mechanisms of facial expression, the recognition of emotions across cultures, and the use of facial expression analysis in fields such as lie detection and robotics. Several datasets for the study of facial expressions are also mentioned.

The text includes references to various sources discussing the Facial Action Coding System (FACS) developed by Ekman and Friesen, as well as debates around the nature of emotions and the reliability of automated emotion recognition technology. The text also references critiques of assumptions about biology and bias in technology, and highlights the complexity and multiple meanings of emotions.

The text includes references to various sources discussing the use of facial recognition and AI technology for surveillance and intelligence-gathering purposes, as well as the development of software and strategies for surveillance and military advantage by government agencies. It also includes references to discussions around the ethics and legality of these practices, as well as the history and development of the internet and military offsets.

The US has relied on nuclear deterrence, espionage, and advanced technology to achieve military superiority, with the Second Offset strategy in the 1970s-80s emphasizing computational analytics and precision-guided weapons. Russia and China's adoption of similar capabilities has led to renewed focus on strategic advantage, with tech companies like Google and Microsoft partnering with the US military on projects such as AI for drones.

The award of the JEDI contract to Microsoft was due to its engineering capabilities, rather than just a sales opportunity. Tech companies such as Google and Palantir have faced ethical concerns over their involvement with military and law enforcement agencies using their AI and data analytics technologies. The migration of law enforcement to intelligence was happening before the shift to predictive analytics, and court decisions have made it easier for law enforcement to engage in surveillance.

The text is a list of numbered references and citations covering various topics related to the use of surveillance technologies, including their impact on law enforcement, data collection and analysis, and the role of technology in social regulation and inequality. The references include books, articles, and news reports, some of which address the use of technology by government agencies and private companies for purposes such as targeted killing and immigration enforcement. The text also notes the uncertain future of the Snowden archive.

The text notes that the news organization The Intercept will no longer fund the Snowden archive, which contains classified documents leaked by Edward Snowden. The rest of the text is a numbered list of references covering various topics related to artificial intelligence, including game playing and the concept of intelligence, the myths surrounding AI, the ontology of the enemy, and the idea of superintelligence. The references also touch on issues related to labor and workers' rights in the tech industry.

The text includes references to various sources on topics such as the idea of data as capital, algorithmic warfare, AI ethics guidelines, and space exploration. The sources cited touch on themes such as the limitations of AI ethics frameworks, the potential for harm in technology, and the need for decolonial perspectives in AI. There are also references to articles about Jeff Bezos' and Elon Musk's plans for space exploration.

The text briefly mentions various sources and perspectives regarding space colonization and resource consumption, including criticism of optimistic models, advocacy for a no-growth model, and a science fiction story. It also cites legal and political developments in space exploration and commercialization.

The text includes a list of various sources and their citations, covering topics such as technological immortality, automation, the dark side of space exploration, the rare metal age, lithium industry in Bolivia, US drone strikes, and artificial intelligence. The sources range from books to articles and websites.

The text includes a list of various sources and their citations covering topics related to artificial intelligence, including worker surveillance, automated hiring platforms, and face analysis technologies such as Amazon Rekognition. The sources range from academic papers to news articles and company websites.

The given text includes various sources and topics related to communication technology and its impacts. It discusses trends in communication technology until 2030 and challenges that may arise. It also includes references to books, articles, and reports discussing topics such as automated media, age discrimination in job ads, machine bias in criminal sentencing, Palantir technology used by ICE to detain immigrants, Apple's commitment to be carbon neutral by 2030, and the assessment of the Dodd-Frank Act on conflict minerals.

The text includes various sources on different topics such as conflict minerals, mechanical turk, philosophy of science, speech recognition, emotions, and artificial intelligence on social media.

The text consists of a list of various sources with their respective titles, authors, and publication dates, covering topics such as defining emotions, electric car batteries, ICT emissions, race and technology, drone strikes, machine learning, digital labor platforms, Pentagon's use of Google AI for drone programs, and Jeff Bezos' speech on space exploration.

The text is a list of various sources and their corresponding titles, covering a range of topics such as space exploration, facial recognition, social class determination, eugenics, and classification in the sciences. It includes books, technical reports, articles, and videos.

The text includes a list of books and articles on various topics, including labor, surveillance, technology, and military contracts. The sources range from historical accounts to current debates surrounding issues such as biometrics, artificial intelligence, and the role of technology companies in government contracts.

The text is a list of books, articles, and reports that cover a variety of topics related to technology, including gender and racial biases in AI, energy consumption, and government contracts for AI and drone technology. It also includes historical accounts and discussions on the ethics and responsibility of artificial intelligence.

The text includes various sources covering topics such as the water usage of NSA Utah Data Center, the race for AI strategic advantage, Chinese lithium supply to Tesla, the history of facial recognition, and books on epistemic cultures and the ecology of attention.

The text includes various academic articles, news reports, and book talks on diverse topics, including neurology, climate change, epistemology, legal constructions of informational capitalism, and disease classification related to sexual orientation in the International Statistical Classification of Diseases and Related Health Problems. The text also discusses the proposed declassification of disease categories related to sexual orientation, the costs of connection with data colonization, and media, technology, and literature in the 19th century.

This is a list of various sources with a range of topics, including crisis reporting, the use of facial recognition technology in schools, the environmental impact of lost shipping containers, the development of Google's globally-distributed database, and discussions on data colonialism and design justice.

The text is a list of various sources, including books, reports, and articles, on topics related to technology, artificial intelligence, climate change, data privacy, facial recognition, and displacement and resistance. Some of the sources include recommendations for policy changes, caution against potential harm, and explore the historical context and development of technology.

The given text is a list of various sources covering topics such as technology, space exploration, judicial decisions, bias in AI recruiting tools, the expression of emotions, objectivity, anatomy of passions, and artificial intelligence. It includes sources such as research papers, news articles, books, and videos.

This is a list of various sources and topics, including the ImageNet database, China's Next Generation AI Development Plan, the establishment of the Algorithmic Warfare Cross-Functional Team, machine learning bias, data feminism, and the connection between AI and climate change.

The provided text does not have a coherent or relevant topic, but rather appears to be a list of various references and sources on different topics, including AI and climate change, machine learning, the Fukushima nuclear disaster trial, Google Photos' image labeling issues, depathologizing homosexuality, and AI cameras for shoplifting prevention, among others.

The text lists several academic publications on topics including the discourse of Cold War America, the technopolitics of identity in apartheid South Africa, the master-slave analogy in technical literature, basic emotions and facial expressions, nonverbal communication and deception, and the Facial Action Coding System. Many of the publications were written by Paul Ekman and cover his research on facial expressions and emotions.

This is a list of references related to the study of facial expressions, emotion recognition, and related technologies. The references include books, articles, and market reports on topics such as the universality and cultural specificity of emotion recognition, methods for studying spontaneous facial expression, and the use of emotion detection and recognition technology.



The article contains a list of sources on various topics related to technology, including facial recognition, automation, artificial intelligence, and labor issues. 

One article discusses the high failure rate of facial recognition technology used by the UK Met police. 

Another article explores the history of computers and the politics of technical expertise. 

A book titled "Automating Inequality" examines how high-tech tools can profile, police, and punish the poor. 

An AI service called "Face" is described as a tool for analyzing faces in images. 

A patent application describes a smart-home automation system that suggests or implements household policies based on sensed observations. 

The sources also touch on issues such as the problematic use of lie detectors and the collection and use of human remains for scientific research.

The text consists of a list of various sources covering topics such as leaked emails regarding Google's military drone AI work, federal policies for human subject protection, the book "Wages Against Housework", the WordNet database, the science of facial expression, Palantir's relationship with government agencies, the Five Eyes Intelligence Oversight and Review Council, Jeff Bezos' master plan, and the construction of knowledge in artificial intelligence.

The text includes various sources discussing topics such as robotization and the domestic sphere, surveillance as social regulation, facial displays, and data privacy. The sources cited include academic articles, books, and news articles.

The given text is a list of various sources including articles, books, and reports. It covers topics such as vision, knowledge, biometrics, tax incentives, datasets, homelessness, intelligence mining, shipping, and AI research.

The provided text is a list of various sources and references, with no clear overarching theme or topic.

The text includes various sources on topics such as technology, AI, bias, time management, lithium extraction, and face recognition. It includes books, articles, and websites, and covers topics ranging from the history of German physiognomic thought to current legal issues in the fast-food industry. The sources are not connected by a central theme or argument.

The text contains a list of various sources and articles related to topics such as AI, working conditions, surveillance, ethics, and targeted killing policies. Each source or article is listed with its title, author, and publication information.

The text is a list of various sources on topics such as AI and natural language processing, China's AI industry, feminist literature, carbon footprint reduction, AI algorithms for hiring and surveillance, and Amazon's home security company Ring. It does not provide any coherent narrative or analysis.

The text contains various articles covering topics such as facial recognition research, the Enron scandal, women in computing, tin mining in Indonesia, and the negative effects of bad engineering choices. It also includes an article discussing how facial expressions are not always reliable indicators of emotions, and another article that uses various types of analysis to claim that a politician is a compulsive liar.

The text consists of a list of various sources and their titles, covering topics such as Ilhan Omar, data centers, military intervention, surveillance, brain size and intelligence, lithium-ion batteries, chatbots, artificial intelligence, cloud computing, and responsible minerals supply chain. No further information or connections are provided.

The text consists of a list of various academic sources on topics related to technology, including AI ethics, biometrics regulation, emotion recognition, automation, conflict minerals, digital image processing, and more.

The text consists of a list of various academic publications, including conference proceedings, journal articles, and news reports, covering topics such as affect detection, alternative work arrangements, human-computer interaction, and machine learning.

The text is a list of various academic sources, including books, articles, and research papers, covering topics such as AI, race, language, material science, and more. The sources are listed with their author, title, publication information, and a link (where applicable) to access them. No further information or context is provided.

This is a list of sources on various topics, including a media history of text prediction, concerns about a secretive state gang database, the history of women in computing, rare earth deposits in China, the classification of animals, the false monopoly of China in the rare earths trade, the deletion of face databases by Microsoft, and a quote by Audre Lorde.

The text is a list of references from various sources covering topics such as practical economies, facial expression coding, workplace surveillance, annotated corpus of English, nuclear missile guidance, artificial intelligence in the military, and web image search technology.

The text contains a list of various sources on topics such as automation, robotics, energy consumption in data centers, sales of Alexa devices, ethics of AI, and occupational health and safety risks in the fast food industry. It includes sources from authors such as Karl Marx and Shannon Mattern, as well as news articles from various publications like the New York Times, NBC News, and BBC Future.

The text includes references to various topics, including the experiences of young temporary workers, the growth of biological thought, necropolitics, the history of artificial intelligence, the Fukushima nuclear disaster, facial expression datasets, the intersection of race and technology, and media theory.

This is a list of references, so no summary can be provided without additional information on what the text is about.

The text is a list of various sources, including academic papers, books, and news articles, on topics related to artificial intelligence, machine learning, and computer vision. The sources cover topics such as the ethics of AI, the history of computer vision, and the use of AI in workplace surveillance.

The text consists of a list of various sources and their titles, including articles and books on topics such as child exploitation on YouTube, facial recognition technology, the NSA, and the mineral commodity supply risk of the US manufacturing sector, among others. It also includes the Outer Space Treaty of 1967 and a collaboration between Amazon and NSF to accelerate fairness in AI. No further information or context is provided.

The text includes a list of various sources, including articles, books, and databases related to topics such as fairness in AI research, battery power ratings, gender bias in financial services, racism in science, and ICE surveillance. The sources are from various publishers and authors, including Amazon, NIST, CNN, New York Times, Beacon Press, and Friedrich Nietzsche.

The text consists of a list of various sources and their corresponding titles, including books, articles, and websites, covering topics such as artificial intelligence, the use of AI in recruitment, algorithmic oppression, data privacy, space exploration, and legal cases related to the Fukushima nuclear disaster.

The text is a list of various sources related to topics such as artificial intelligence, algorithms, data privacy, and technology in general. It includes sources such as books, articles, videos, and websites covering subjects ranging from the history of AI to the impact of data on democracy and inequality, and the use of AI in military applications.

The provided sources cover a wide range of topics, including face recognition technology, affective computing, AI principles, feminist logic, deep vs shallow networks, lie detection, space colonies, renewable energy for the cloud, arts of the contact zone, NSA growth, and transculturation.

The text includes various sources related to topics such as artificial intelligence, digital abolition, big data, bioinformationalization, and labor in the iPhone era. It also discusses issues related to privacy, security, biopower, and ethics in technology, including facial recognition auditing, biased performance results of AI products, and company e-mail policies. The sources cited include books, articles, reports, and meeting minutes from various authors, organizations, and publishers.

The text contains a list of various sources on different topics related to computing, including a book on the history of computing in the US, a blog post about women in the Mathematical Tables Project, an article on Taylorism and Fordism in the stockyards, a magazine article on the first photo from space, a research paper on the collision probabilities of cars with planets, responsible minerals policies of companies like Philips and Dell, a news article on Google DeepMind's NHS data deal, a research paper on racial influence on automated perceptions of emotions, and a report on litigating algorithms by the AI Now Institute.



The texts cover a range of topics including the power and goals of the NSA, sexism and racism in future Mars colonies, the re-creation of race in science, content moderation on social media, legal challenges against Amazon's Alexa, the use of social media by foreign travelers, deep learning for human affect recognition, and cross-cultural studies on emotion recognition.

 The sources cited include The New York Times, The Guardian, The Seattle Times, Politico, and various academic journals.

 The texts address a variety of legal and social issues related to technology, including privacy, bias, discrimination, and the role of open standards in the digital age.

The text is a list of various sources, including academic papers, books, and news articles, on topics such as artificial intelligence, datafication, workplace surveillance, facial recognition, and space settlements. The sources cover a range of perspectives and issues related to technology and society.



Eric Schmidt highlights that the US is lagging behind China in terms of AI and that the country needs to take steps to ensure its continued dominance in the field. 

Algorithms used for facial analysis have been trained using image databases that construct race and gender, perpetuating bias and reinforcing societal stereotypes. 

There is a global shortage of minerals needed for electric vehicle batteries, which may pose a challenge to the widespread adoption of electric vehicles. 

Shipping is a significant contributor to greenhouse gas emissions and needs to be addressed to mitigate the effects of climate change. 

There are concerns about the use of facial recognition software and the potential for machines to read emotions, with implications for privacy and individual freedoms. 

The NSA targets users of online anonymity tools, such as Tor, compromising individual privacy and security. 

Scholars have critiqued certain schemes aimed at improving the human condition, highlighting the potential for unintended consequences and negative impacts.

The text is a list of various sources with their respective titles, authors, and publication information, covering topics such as disability theory, video surveillance with face recognition, emotion analysis, asteroid mining, and business ethics. Each source has a DOI or link to the publication.

The text is a list of various sources with their respective titles, authors, and publication information, covering topics such as responsible research with crowdsourcing, drone surveillance, carbon negativity, license plate reader technology, and online job ad targeting. The sources also include a classic economic work and documents related to US military technology and surveillance. Each source has a DOI or link to the publication.

The provided text is a list of various sources on different topics, including facial recognition technology, philosophy, history, AI ethics, cultural analytics, and business news. There is no cohesive narrative or connection between the sources.

The text is a collection of various sources and does not have a clear central theme. Therefore, a concise summary in key points cannot be provided.

The given text appears to be a list of various sources and references, including articles, books, and research papers, on a wide range of topics such as automation, management, human populations, consciousness, identity, and counseling, among others. There is no coherent theme or narrative presented in the text.

The text contains a list of sources and references, covering topics such as renewable energy, supply chain analytics, sound technology, artificial intelligence, telegraph, geology, affective computing, and more. There is no clear overarching theme or argument presented in the text.

The given text is a list of various sources ranging from academic articles, government documents, news articles, and websites, covering topics such as AI emotion recognition, shipping pollution, electricity generation, drones, and more.

The text is a list of references cited by an unknown source, covering topics such as Tesla's environmental impact, the influence of Silicon Valley on time, the use of temp workers at Google, Palantir's data collection, deep neural networks, immigration policy and technology, Wernher von Braun, and the intersection of computer power and human reasoning.

The text is a list of various sources, including academic articles, reports, news articles, and websites, covering topics such as the impact of computers on society, Elon Musk's plans for a Mars mission, the architecture of absolutism in 18th century Russia, modern racism, fairness and bias in AI, disability and AI, asteroid mining, innovations in shipping boxes, lobbying victories in 2015, and predictive policing.

The text contains a list of sources related to technology and its impact on society, including topics such as predictive policing, AI bias, surveillance capitalism, and labor practices in the AI industry in China. Some sources discussed in the text include academic papers, news articles, and datasets related to computer vision and machine learning.

This text is a list of various names, organizations, and concepts related to the field of artificial intelligence. It includes discussions of affect recognition and algorithmic assessment, as well as mentions of specific companies such as Amazon and Apple. The text also touches on issues such as bias in algorithms, labor practices, and surveillance.

The text lists various topics, people, and events related to the intersection of technology, power structures, and societal issues, including AI principles, mineral extraction, bias in algorithms, state power, affect recognition, and data surveillance by government agencies and corporations. Some notable figures mentioned include Charles Babbage, Jeff Bezos, Jeremy Bentham, and Vannevar Bush. The text also references various studies and events in different countries, including lithium mining in Australia, Bolivia, and Mongolia, affect recognition studies in Brazil, and the CalGang database in the United States.

The text discusses various topics related to AI, including its development in China, data surveillance and collection in China, and rare earth mineral extraction. It also covers issues such as cognitive bias, conflict minerals, and the carbon footprint of cloud computing and data centers. The article explores the limits and biases of data classification, particularly in the areas of gender and racial bias, and highlights the challenges of debiasing systems. Other topics covered include colonialism, credit scoring, crime classification algorithms, and the role of AI in the criminal justice system.

The text discusses issues related to data, including overfitting, data colonialism, and data extraction. It also covers topics such as algorithmic assessment, biometrics, and privacy issues. The role of state power and the history of data demand are discussed, as well as the impact of technology on labor efficiency and the environment. The text touches on various individuals and companies, including DeepMind, Palantir, and Dominos Pizza. Additionally, it covers topics such as facial recognition algorithms, myths and metaphors of data, and the ethics of using data.

The text discusses the impacts and ethical implications of AI in various industries, including cargo containers, data centers, electric vehicles, e-waste dumping grounds, gold and silver mining, latex and lithium extraction, rare earth minerals, submarine cable construction, and water supply. It explores how AI can be used for data and labor extraction, as well as affect and facial recognition. The text also references various individuals and organizations, including Jeffrey Epstein, Virginia Eubanks, Facebook, the FBI, and Palantir.

The text is a list of names, concepts, and organizations related to technology, science, and society, including individuals such as Michel Foucault, Timnit Gebru, and Grace Hopper, companies like Google and Goldman Sachs, and topics such as data bias, artificial intelligence, and mining. It also includes references to specific works, events, and programs, such as Discipline and Punish, Project Maven, and the FOXACID program.

The text contains a variety of topics including labor, data surveillance, affect recognition, and institutional review boards. It discusses the goals and training datasets for ImageNet, the use of AI in labor including algorithmic scheduling and surveillance, and the exploitation of workers. It also mentions the mining industry in Indonesia, the Joint Enterprise Defense Infrastructure, and the International Labour Organization. The role of AI in justice and the connection between data classification and climate justice are also discussed.

This is a list of various people, organizations, and topics related to the use of artificial intelligence, including the extraction industry, law enforcement, the carbon footprint of data centers, facial recognition, affect recognition research, and mineral extraction. It includes references to specific individuals such as Cesare Lombroso and Marshall McLuhan, as well as organizations such as Microsoft and Lockheed Martin, and concepts like the myth of clean tech and the microphysics of power.

This text consists of a list of various topics, including lectures on management and the future of computers, lithium mining and rare earth mineral extraction in Mongolia, data collection and surveillance by entities such as the National Security Agency and Palantir, and the use of artificial intelligence models such as natural language processing. It also includes references to individuals such as Elon Musk and Safiya Umoja Noble, as well as various books and treaties.

The text includes various topics such as phrenology, physiognomy, power structures, race, privacy issues, and AI development in Silicon Valley. It also mentions specific companies and programs like Planetary Resources, Ring doorbell cameras, and Project Maven, as well as individuals like Rosalind Picard and Eric Schmidt. The text touches on the use of AI in law enforcement and surveillance systems and the impact of the tech industry on cities like San Francisco. Additionally, it discusses the role of time coordination in various industries and the collection of rare earth minerals in places like Russia.

This is a list of various topics and individuals mentioned in a text, including companies, individuals, countries, technologies, and concepts. The text covers a wide range of subjects, including mining, literature, AI, surveillance, state power, space colonization, supply chains, and more. Some of the notable individuals mentioned include Adam Smith, Peter Thiel, and Upton Sinclair. The text also includes references to specific programs, such as Project Maven and TREASUREMAP, as well as specific technologies, such as speech recognition and social credit scoring.

This is a list of various individuals, organizations, and concepts mentioned in a text, covering a wide range of topics, including technology, data classification, supply chains, and surveillance capitalism. Some of the notable individuals mentioned include Alan Turing, Norbert Wiener, and Shoshana Zuboff. The text also includes references to specific companies, such as Uber and WeWork, as well as specific technologies, such as TrueTime and WISARD.

